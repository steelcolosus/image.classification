{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faed12aca20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 1\n",
    "    \n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (x - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    from sklearn import preprocessing\n",
    "    labels = np.array(x)\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(labels)\n",
    "    lb.classes_ = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "    return lb.transform(labels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None,image_shape[0],image_shape[1],image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return  tf.placeholder(tf.float32, shape=[None]+[n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return  tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    channels = int(x_tensor.get_shape()[3])\n",
    "    #Weights\n",
    "    weights = tf.Variable(tf.truncated_normal([*conv_ksize, channels, conv_num_outputs ]))\n",
    "    #Bias\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    #Apply convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, [1, *conv_strides, 1], padding='SAME')\n",
    "    \n",
    "    #Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    \n",
    "    #Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #Apply max pooling\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "        conv_layer,\n",
    "        ksize = [1,*pool_ksize,1],\n",
    "        strides = [1, *pool_strides, 1],\n",
    "        padding ='SAME'\n",
    "    )\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    height = int(x_tensor.get_shape()[1])\n",
    "\n",
    "    width = int(x_tensor.get_shape()[2])\n",
    "\n",
    "    depth = int(x_tensor.get_shape()[3])\n",
    "    \n",
    "    flat_size = height*width*depth\n",
    "    \n",
    "    \n",
    "    return tf.reshape(x_tensor, [-1, flat_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([int(x_tensor.get_shape()[1]), num_outputs], mean = 0.0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    f_connected = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    f_connected = tf.nn.relu(f_connected)\n",
    "    return f_connected\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([int(x_tensor.get_shape()[1]), num_outputs], mean = 0.0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    output = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 32\n",
    "    conv_ksize = (2,2)\n",
    "    conv_strides = (2,2)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    \n",
    "    \n",
    "    conv1 = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    \n",
    "    conv_num_outputs2 = 16\n",
    "    conv_ksize2 = (2,2)\n",
    "    conv_strides2 = (2,2)\n",
    "    pool_ksize2 = (2,2)\n",
    "    pool_strides2 = (2,2)\n",
    "    \n",
    "    conv2 = conv2d_maxpool(x_tensor, conv_num_outputs2, conv_ksize2, conv_strides2, pool_ksize2, pool_strides2)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_layer = flatten(conv2)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    f_connected = fully_conn(flat_layer, 350)\n",
    "    f_connected = tf.nn.dropout(f_connected, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    output_layer = output(f_connected, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    current_acurracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    result = 'Loss {} Accurracy {}'.format(loss, current_acurracy)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss 2.1806461811065674 Accurracy 0.21619999408721924\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss 2.095092535018921 Accurracy 0.24500000476837158\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss 2.0240817070007324 Accurracy 0.2691999673843384\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss 1.978149175643921 Accurracy 0.2919999957084656\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss 1.9344698190689087 Accurracy 0.32120001316070557\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss 1.9009883403778076 Accurracy 0.3203999698162079\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss 1.8574652671813965 Accurracy 0.34919998049736023\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss 1.8336575031280518 Accurracy 0.3571999669075012\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss 1.7882821559906006 Accurracy 0.3702000081539154\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss 1.7427996397018433 Accurracy 0.3853999376296997\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss 1.7217464447021484 Accurracy 0.384799987077713\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss 1.679284930229187 Accurracy 0.3985999822616577\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss 1.6375936269760132 Accurracy 0.40539997816085815\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss 1.6169040203094482 Accurracy 0.42080000042915344\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss 1.5873208045959473 Accurracy 0.42479997873306274\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss 1.5572630167007446 Accurracy 0.43779999017715454\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss 1.5264370441436768 Accurracy 0.44519996643066406\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss 1.4937349557876587 Accurracy 0.4367999732494354\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss 1.4878791570663452 Accurracy 0.45239996910095215\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss 1.4525203704833984 Accurracy 0.4591999650001526\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss 1.4339344501495361 Accurracy 0.4609999656677246\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss 1.398137092590332 Accurracy 0.45959997177124023\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss 1.3813740015029907 Accurracy 0.4649999737739563\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss 1.3574037551879883 Accurracy 0.46939992904663086\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss 1.331734299659729 Accurracy 0.47519993782043457\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss 1.3027777671813965 Accurracy 0.47419998049736023\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss 1.2644016742706299 Accurracy 0.48479998111724854\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss 1.271223783493042 Accurracy 0.4851999878883362\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss 1.2535889148712158 Accurracy 0.49199995398521423\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss 1.2142177820205688 Accurracy 0.49699997901916504\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss 1.2038753032684326 Accurracy 0.48919999599456787\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss 1.1892694234848022 Accurracy 0.4987999498844147\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss 1.1817312240600586 Accurracy 0.4939999580383301\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss 1.1525036096572876 Accurracy 0.49619990587234497\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss 1.1376068592071533 Accurracy 0.4955999553203583\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss 1.103841781616211 Accurracy 0.5013999342918396\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss 1.0961447954177856 Accurracy 0.5069999098777771\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss 1.071556568145752 Accurracy 0.5105999708175659\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss 1.0572998523712158 Accurracy 0.5109999179840088\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss 1.0466511249542236 Accurracy 0.5085999369621277\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss 1.0302114486694336 Accurracy 0.5125999450683594\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss 0.9886527061462402 Accurracy 0.5162000060081482\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss 0.9755403995513916 Accurracy 0.507599949836731\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss 0.9586669206619263 Accurracy 0.5139999985694885\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss 0.9619814157485962 Accurracy 0.515799880027771\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss 0.9288109540939331 Accurracy 0.5153999328613281\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss 0.9153095483779907 Accurracy 0.5193999409675598\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss 0.9036039113998413 Accurracy 0.5209999680519104\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss 0.892303466796875 Accurracy 0.5267999768257141\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss 0.8664669990539551 Accurracy 0.5257999300956726\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss 0.8630754947662354 Accurracy 0.5267999768257141\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss 0.8313880562782288 Accurracy 0.5269999504089355\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss 0.8383656740188599 Accurracy 0.5247998833656311\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss 0.8087542057037354 Accurracy 0.5305999517440796\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss 0.7983154058456421 Accurracy 0.5243999361991882\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss 0.7958431243896484 Accurracy 0.5299999117851257\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss 0.77042156457901 Accurracy 0.527999997138977\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss 0.7609492540359497 Accurracy 0.5279999375343323\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss 0.7526915669441223 Accurracy 0.5265999436378479\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss 0.7574211955070496 Accurracy 0.5227999091148376\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss 0.7381304502487183 Accurracy 0.5279999375343323\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss 0.7112006545066833 Accurracy 0.5399999618530273\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss 0.7098501920700073 Accurracy 0.5329999327659607\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss 0.6862819194793701 Accurracy 0.5363999009132385\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss 0.6782311797142029 Accurracy 0.5335999727249146\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss 0.677824079990387 Accurracy 0.539199948310852\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss 0.6690422296524048 Accurracy 0.5401999354362488\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss 0.6594244837760925 Accurracy 0.5293999910354614\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss 0.6474699378013611 Accurracy 0.5383999943733215\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss 0.6377394199371338 Accurracy 0.540399968624115\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss 0.6105114817619324 Accurracy 0.5365999341011047\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss 0.5961802005767822 Accurracy 0.5419999361038208\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss 0.5943833589553833 Accurracy 0.5445999503135681\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss 0.591454029083252 Accurracy 0.545199990272522\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss 0.5844607353210449 Accurracy 0.5433999300003052\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss 0.5509815812110901 Accurracy 0.5441999435424805\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss 0.5359489917755127 Accurracy 0.540199875831604\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss 0.5221455097198486 Accurracy 0.5435999035835266\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss 0.5156158804893494 Accurracy 0.5441999435424805\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss 0.507857084274292 Accurracy 0.5437999367713928\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss 0.51303631067276 Accurracy 0.5439999103546143\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss 0.4993859529495239 Accurracy 0.5395999550819397\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss 0.5023844838142395 Accurracy 0.5389999747276306\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss 0.47554922103881836 Accurracy 0.5479999780654907\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss 0.46159273386001587 Accurracy 0.5437999367713928\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss 0.467033714056015 Accurracy 0.5449999570846558\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss 0.44763585925102234 Accurracy 0.543999969959259\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss 0.43903589248657227 Accurracy 0.5447999238967896\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss 0.43489402532577515 Accurracy 0.544999897480011\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss 0.4107937216758728 Accurracy 0.5449999570846558\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss 0.4110705256462097 Accurracy 0.5477998852729797\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss 0.39979302883148193 Accurracy 0.547999918460846\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss 0.3982568383216858 Accurracy 0.5445998907089233\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss 0.38415834307670593 Accurracy 0.5503999590873718\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss 0.386355459690094 Accurracy 0.5417999029159546\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss 0.3757786750793457 Accurracy 0.5451999306678772\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss 0.3718549907207489 Accurracy 0.5495999455451965\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss 0.36953070759773254 Accurracy 0.5487999320030212\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss 0.3526453375816345 Accurracy 0.547999918460846\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss 0.344403475522995 Accurracy 0.5541999340057373\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss 2.2375059127807617 Accurracy 0.20239999890327454\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss 2.057204484939575 Accurracy 0.2441999912261963\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss 1.9837826490402222 Accurracy 0.2921999990940094\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss 1.859879970550537 Accurracy 0.29979997873306274\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss 1.8676950931549072 Accurracy 0.3240000009536743\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss 1.8971598148345947 Accurracy 0.3591999411582947\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss 1.736569881439209 Accurracy 0.35600000619888306\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss 1.7141146659851074 Accurracy 0.36479994654655457\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss 1.6704020500183105 Accurracy 0.3830000162124634\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss 1.694566011428833 Accurracy 0.3933999836444855\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss 1.7690707445144653 Accurracy 0.4049999713897705\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss 1.593935251235962 Accurracy 0.41099995374679565\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss 1.5657538175582886 Accurracy 0.41759994626045227\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss 1.5426957607269287 Accurracy 0.4174000024795532\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss 1.59149968624115 Accurracy 0.43540000915527344\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss 1.679396390914917 Accurracy 0.4371999502182007\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss 1.5053843259811401 Accurracy 0.44679996371269226\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss 1.4884980916976929 Accurracy 0.4406000077724457\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss 1.4452197551727295 Accurracy 0.4487999677658081\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss 1.5422232151031494 Accurracy 0.45659998059272766\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss 1.613933801651001 Accurracy 0.4631999731063843\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss 1.44682776927948 Accurracy 0.4641999900341034\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss 1.4113203287124634 Accurracy 0.45879998803138733\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss 1.3587510585784912 Accurracy 0.4729999601840973\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss 1.4596315622329712 Accurracy 0.4803999662399292\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss 1.5533299446105957 Accurracy 0.4789999723434448\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss 1.3802647590637207 Accurracy 0.48399996757507324\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss 1.328690767288208 Accurracy 0.4829999506473541\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss 1.320819616317749 Accurracy 0.48819994926452637\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss 1.4189211130142212 Accurracy 0.48659995198249817\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss 1.493155837059021 Accurracy 0.4973999559879303\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss 1.3413655757904053 Accurracy 0.4957999587059021\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss 1.2946938276290894 Accurracy 0.4915999472141266\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss 1.2708685398101807 Accurracy 0.5057999491691589\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss 1.361142635345459 Accurracy 0.5031999349594116\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss 1.440065622329712 Accurracy 0.5131999254226685\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss 1.3006199598312378 Accurracy 0.51419997215271\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss 1.2323309183120728 Accurracy 0.49919992685317993\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss 1.2325326204299927 Accurracy 0.5131999850273132\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss 1.3097888231277466 Accurracy 0.5229999423027039\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss 1.3984324932098389 Accurracy 0.5141999125480652\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss 1.251903772354126 Accurracy 0.5023999810218811\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss 1.209366798400879 Accurracy 0.5175999402999878\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss 1.2036131620407104 Accurracy 0.5187999606132507\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss 1.2723811864852905 Accurracy 0.5191999673843384\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss 1.3653819561004639 Accurracy 0.5233999490737915\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss 1.2019119262695312 Accurracy 0.5231999158859253\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss 1.1816545724868774 Accurracy 0.5245999097824097\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss 1.1665946245193481 Accurracy 0.5281999111175537\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss 1.2304927110671997 Accurracy 0.5353999733924866\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss 1.3152190446853638 Accurracy 0.5311999320983887\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss 1.173142433166504 Accurracy 0.5327999591827393\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss 1.1505541801452637 Accurracy 0.5341998934745789\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss 1.132856011390686 Accurracy 0.5357999205589294\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss 1.218155860900879 Accurracy 0.5389999151229858\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss 1.2800980806350708 Accurracy 0.5389999151229858\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss 1.1358128786087036 Accurracy 0.5365999341011047\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss 1.136561632156372 Accurracy 0.5415999293327332\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss 1.1220684051513672 Accurracy 0.5411999821662903\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss 1.1579656600952148 Accurracy 0.5489999055862427\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss 1.2402184009552002 Accurracy 0.5497999787330627\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss 1.1046810150146484 Accurracy 0.5457999110221863\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss 1.0954945087432861 Accurracy 0.5469999313354492\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss 1.0845544338226318 Accurracy 0.5535999536514282\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss 1.1402616500854492 Accurracy 0.5527999401092529\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss 1.2181884050369263 Accurracy 0.5469999313354492\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss 1.086329460144043 Accurracy 0.5519999265670776\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss 1.0759254693984985 Accurracy 0.550399899482727\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss 1.0556501150131226 Accurracy 0.5539999008178711\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss 1.1083701848983765 Accurracy 0.5565999746322632\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss 1.190705418586731 Accurracy 0.5645999908447266\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss 1.0570218563079834 Accurracy 0.5577999353408813\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss 1.05314040184021 Accurracy 0.5551999807357788\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss 1.0459942817687988 Accurracy 0.5583999156951904\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss 1.0545573234558105 Accurracy 0.5647999048233032\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss 1.157188057899475 Accurracy 0.5667999386787415\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss 1.0531015396118164 Accurracy 0.5591999292373657\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss 1.0306594371795654 Accurracy 0.5663999319076538\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss 1.0199710130691528 Accurracy 0.5687999129295349\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss 1.0273683071136475 Accurracy 0.5697999000549316\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss 1.148673415184021 Accurracy 0.571199893951416\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss 1.0016666650772095 Accurracy 0.5689998865127563\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss 0.9972323775291443 Accurracy 0.5699999332427979\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss 1.0026048421859741 Accurracy 0.5651999115943909\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss 1.0133724212646484 Accurracy 0.5735999345779419\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss 1.110044002532959 Accurracy 0.56659996509552\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss 1.0046184062957764 Accurracy 0.5713999271392822\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss 0.9885141849517822 Accurracy 0.5743999481201172\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss 0.9737396240234375 Accurracy 0.5735999345779419\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss 1.0071923732757568 Accurracy 0.5753999352455139\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss 1.0693130493164062 Accurracy 0.58079993724823\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss 0.9697099924087524 Accurracy 0.572399914264679\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss 0.9800374507904053 Accurracy 0.5793999433517456\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss 0.9629600048065186 Accurracy 0.5771999359130859\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss 0.9714523553848267 Accurracy 0.5813999176025391\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss 1.0566414594650269 Accurracy 0.5789998769760132\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss 0.947638988494873 Accurracy 0.5853999257087708\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss 0.9453692436218262 Accurracy 0.5801998972892761\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss 0.9402939677238464 Accurracy 0.5794000029563904\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss 0.9467009902000427 Accurracy 0.5879999399185181\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss 1.0289552211761475 Accurracy 0.5905998945236206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, CIFAR-10 Batch 2:  Loss 0.9368940591812134 Accurracy 0.5889999270439148\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss 0.9319463968276978 Accurracy 0.5847999453544617\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss 0.9195643663406372 Accurracy 0.5821999311447144\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss 0.9247792959213257 Accurracy 0.5917999148368835\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss 1.0039242506027222 Accurracy 0.5865998268127441\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss 0.896304190158844 Accurracy 0.5855998992919922\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss 0.9332960844039917 Accurracy 0.5897998809814453\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss 0.8973874449729919 Accurracy 0.5905999541282654\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss 0.9079006314277649 Accurracy 0.5931999087333679\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss 0.9924836754798889 Accurracy 0.5791999101638794\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss 0.8804240822792053 Accurracy 0.5903998613357544\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss 0.8961058855056763 Accurracy 0.5891999006271362\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss 0.8836156725883484 Accurracy 0.5897999405860901\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss 0.8888776898384094 Accurracy 0.5933998823165894\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss 0.9557315111160278 Accurracy 0.593999981880188\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss 0.8676260709762573 Accurracy 0.5893998146057129\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss 0.8664093613624573 Accurracy 0.5891999006271362\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss 0.8640693426132202 Accurracy 0.5905998945236206\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss 0.8731726408004761 Accurracy 0.5879999399185181\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss 0.9422535300254822 Accurracy 0.5975999236106873\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss 0.8625490665435791 Accurracy 0.5969999432563782\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss 0.8464441895484924 Accurracy 0.5939999222755432\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss 0.8541960120201111 Accurracy 0.5915999412536621\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss 0.836896538734436 Accurracy 0.5989999175071716\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss 0.9128565788269043 Accurracy 0.5991999506950378\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss 0.8349047899246216 Accurracy 0.5971999168395996\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss 0.8229987621307373 Accurracy 0.5995999574661255\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss 0.8305138349533081 Accurracy 0.5981999039649963\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss 0.8365178108215332 Accurracy 0.5989999175071716\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss 0.8899380564689636 Accurracy 0.6029998660087585\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss 0.8086879253387451 Accurracy 0.5977998971939087\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss 0.8052717447280884 Accurracy 0.6023999452590942\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss 0.8127987384796143 Accurracy 0.6029999256134033\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss 0.8013672828674316 Accurracy 0.5989999175071716\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss 0.8700583577156067 Accurracy 0.6001999378204346\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss 0.7924185991287231 Accurracy 0.598599910736084\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss 0.7749719619750977 Accurracy 0.6015998721122742\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss 0.7919938564300537 Accurracy 0.6039999723434448\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss 0.7778847813606262 Accurracy 0.6045998930931091\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss 0.8576049208641052 Accurracy 0.6091998815536499\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss 0.7733110189437866 Accurracy 0.6017998456954956\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss 0.7651588916778564 Accurracy 0.6061999201774597\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss 0.7690416574478149 Accurracy 0.6017999649047852\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss 0.7648949027061462 Accurracy 0.600399911403656\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss 0.8191869258880615 Accurracy 0.6057998538017273\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss 0.7683395743370056 Accurracy 0.6079999208450317\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss 0.7538737058639526 Accurracy 0.608799934387207\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss 0.7575736045837402 Accurracy 0.6043999195098877\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss 0.7522199749946594 Accurracy 0.60999995470047\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss 0.8068649768829346 Accurracy 0.6055999398231506\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss 0.7550389170646667 Accurracy 0.6121999025344849\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss 0.722287654876709 Accurracy 0.6073999404907227\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss 0.7508548498153687 Accurracy 0.6023998856544495\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss 0.7236013412475586 Accurracy 0.611599862575531\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss 0.8001797199249268 Accurracy 0.6081998944282532\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss 0.7308340072631836 Accurracy 0.6105998754501343\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss 0.7018155455589294 Accurracy 0.6125998497009277\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss 0.7316346764564514 Accurracy 0.6023998856544495\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss 0.709226667881012 Accurracy 0.6131999492645264\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss 0.7639583945274353 Accurracy 0.6151999235153198\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss 0.7194660902023315 Accurracy 0.611599862575531\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss 0.679334282875061 Accurracy 0.6187999248504639\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss 0.7134727835655212 Accurracy 0.6109999418258667\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss 0.6888235211372375 Accurracy 0.6159998774528503\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss 0.7624686360359192 Accurracy 0.613599956035614\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss 0.7047231793403625 Accurracy 0.613399863243103\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss 0.6769611835479736 Accurracy 0.6185999512672424\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss 0.6955941915512085 Accurracy 0.6117998957633972\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss 0.6901412010192871 Accurracy 0.6155999302864075\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss 0.7338099479675293 Accurracy 0.6131998896598816\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss 0.6689845323562622 Accurracy 0.6093999147415161\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss 0.6478302478790283 Accurracy 0.6197998523712158\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss 0.6837862133979797 Accurracy 0.6073999404907227\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss 0.6478241086006165 Accurracy 0.6167998909950256\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss 0.7377620339393616 Accurracy 0.6171998381614685\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss 0.6762533783912659 Accurracy 0.6189998984336853\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss 0.6162726879119873 Accurracy 0.619399905204773\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss 0.6798413395881653 Accurracy 0.6145999431610107\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss 0.6450496912002563 Accurracy 0.6199999451637268\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss 0.7062209844589233 Accurracy 0.6217999458312988\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss 0.6390522122383118 Accurracy 0.6179999113082886\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss 0.6075392961502075 Accurracy 0.619399905204773\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss 0.6513241529464722 Accurracy 0.6209998726844788\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss 0.619132936000824 Accurracy 0.6161999106407166\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss 0.7046407461166382 Accurracy 0.6245999336242676\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss 0.6174979209899902 Accurracy 0.6237999200820923\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss 0.5912860631942749 Accurracy 0.6249998807907104\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss 0.6456510424613953 Accurracy 0.6137999296188354\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss 0.6181842088699341 Accurracy 0.6171998977661133\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss 0.6820211410522461 Accurracy 0.6157999038696289\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss 0.6174431443214417 Accurracy 0.6259998679161072\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss 0.5660809278488159 Accurracy 0.6233999133110046\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss 0.6278045773506165 Accurracy 0.6201999187469482\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss 0.5932373404502869 Accurracy 0.6197999119758606\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss 0.6714253425598145 Accurracy 0.6227999329566956\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss 0.5980722308158875 Accurracy 0.6275998950004578\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss 0.5487083196640015 Accurracy 0.6249998807907104\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss 0.6151295304298401 Accurracy 0.6165999174118042\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss 0.5986649990081787 Accurracy 0.6181999444961548\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss 0.6403612494468689 Accurracy 0.6241999268531799\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss 0.5909585952758789 Accurracy 0.6253998875617981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, CIFAR-10 Batch 3:  Loss 0.5404585599899292 Accurracy 0.6231998801231384\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss 0.5921675562858582 Accurracy 0.6223999261856079\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss 0.5644171833992004 Accurracy 0.6215999126434326\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss 0.6450128555297852 Accurracy 0.6223999261856079\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss 0.5739697813987732 Accurracy 0.6261999011039734\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss 0.526695191860199 Accurracy 0.6235998868942261\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss 0.5775521397590637 Accurracy 0.6245999336242676\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss 0.5479331016540527 Accurracy 0.6245998740196228\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss 0.6305469274520874 Accurracy 0.6269999146461487\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss 0.5602917671203613 Accurracy 0.6271998882293701\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss 0.5105231404304504 Accurracy 0.621799886226654\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss 0.5533930063247681 Accurracy 0.6197999119758606\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss 0.5273141860961914 Accurracy 0.6299998760223389\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss 0.6121466159820557 Accurracy 0.6283999085426331\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss 0.5408408045768738 Accurracy 0.6231999397277832\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss 0.489349365234375 Accurracy 0.6269999146461487\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss 0.5557413101196289 Accurracy 0.6167998909950256\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss 0.5294756889343262 Accurracy 0.627799928188324\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss 0.5837907791137695 Accurracy 0.6289999485015869\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss 0.5344535112380981 Accurracy 0.6253998279571533\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss 0.47018080949783325 Accurracy 0.627799928188324\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss 0.5265108346939087 Accurracy 0.6205999255180359\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss 0.5163916945457458 Accurracy 0.6215999126434326\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss 0.5666618943214417 Accurracy 0.6287999153137207\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss 0.51466965675354 Accurracy 0.6305999159812927\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss 0.46159014105796814 Accurracy 0.6285998821258545\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss 0.5125980377197266 Accurracy 0.6245999336242676\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss 0.49458175897598267 Accurracy 0.6271998882293701\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss 0.5650180578231812 Accurracy 0.6263998746871948\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss 0.49813011288642883 Accurracy 0.6301999092102051\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss 0.4396184980869293 Accurracy 0.6313998699188232\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss 0.49507850408554077 Accurracy 0.6255998611450195\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss 0.47664201259613037 Accurracy 0.6253998279571533\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss 0.5481666922569275 Accurracy 0.6279999017715454\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss 0.4890223443508148 Accurracy 0.63239985704422\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss 0.4290042519569397 Accurracy 0.6313998699188232\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss 0.49283838272094727 Accurracy 0.6213998794555664\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss 0.46205368638038635 Accurracy 0.6305999159812927\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss 0.5313467979431152 Accurracy 0.6333999037742615\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss 0.47747868299484253 Accurracy 0.6275998950004578\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss 0.41215717792510986 Accurracy 0.6273999214172363\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss 0.4695749282836914 Accurracy 0.6297998428344727\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss 0.45774132013320923 Accurracy 0.6329998970031738\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss 0.5080538988113403 Accurracy 0.6325998902320862\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss 0.4741992950439453 Accurracy 0.6303998827934265\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss 0.40364396572113037 Accurracy 0.6261999011039734\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss 0.46487757563591003 Accurracy 0.6249998807907104\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss 0.45248132944107056 Accurracy 0.624799907207489\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss 0.5113241672515869 Accurracy 0.6335998773574829\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss 0.4624946713447571 Accurracy 0.6303999423980713\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss 0.4029015302658081 Accurracy 0.6299999356269836\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss 0.45292219519615173 Accurracy 0.6261999011039734\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss 0.44226786494255066 Accurracy 0.6317999362945557\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss 0.4917023181915283 Accurracy 0.6303999423980713\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss 0.45090949535369873 Accurracy 0.629599928855896\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss 0.387481689453125 Accurracy 0.6287998557090759\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss 0.4365459084510803 Accurracy 0.6261998414993286\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss 0.43853065371513367 Accurracy 0.6333999633789062\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss 0.4711439609527588 Accurracy 0.635999858379364\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss 0.43898966908454895 Accurracy 0.6343998908996582\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss 0.3661789000034332 Accurracy 0.63239985704422\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss 0.4304063618183136 Accurracy 0.6291999220848083\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss 0.43105989694595337 Accurracy 0.6323999166488647\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss 0.47171419858932495 Accurracy 0.6355999112129211\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss 0.41889601945877075 Accurracy 0.6333999037742615\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss 0.36381837725639343 Accurracy 0.6317999362945557\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss 0.4127620458602905 Accurracy 0.6275999546051025\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss 0.4020830988883972 Accurracy 0.6339999437332153\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss 0.44121381640434265 Accurracy 0.6369999051094055\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss 0.4049438536167145 Accurracy 0.6341999173164368\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss 0.3464226722717285 Accurracy 0.6351999044418335\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss 0.3991798162460327 Accurracy 0.6375998258590698\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss 0.4049929082393646 Accurracy 0.6333999037742615\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss 0.44856539368629456 Accurracy 0.6381998658180237\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss 0.38920626044273376 Accurracy 0.6335998773574829\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss 0.34457576274871826 Accurracy 0.6311999559402466\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss 0.37965670228004456 Accurracy 0.6337999105453491\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss 0.3780900537967682 Accurracy 0.634199857711792\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss 0.4211520254611969 Accurracy 0.6381999254226685\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss 0.38404735922813416 Accurracy 0.6363999247550964\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss 0.3319242000579834 Accurracy 0.6329998970031738\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss 0.36069512367248535 Accurracy 0.6339998841285706\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss 0.3729299008846283 Accurracy 0.634199857711792\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss 0.4172481298446655 Accurracy 0.6355999112129211\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss 0.3650752007961273 Accurracy 0.6347998976707458\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss 0.33369186520576477 Accurracy 0.6287999153137207\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss 0.3518609404563904 Accurracy 0.6305999159812927\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss 0.37839365005493164 Accurracy 0.6347998976707458\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss 0.3999810218811035 Accurracy 0.6375999450683594\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss 0.37036600708961487 Accurracy 0.6391999125480652\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss 0.31372517347335815 Accurracy 0.6381999254226685\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss 0.3449617624282837 Accurracy 0.6385999321937561\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss 0.36141952872276306 Accurracy 0.6349999308586121\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss 0.38995301723480225 Accurracy 0.6383998990058899\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss 0.3551829159259796 Accurracy 0.637799859046936\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss 0.30465418100357056 Accurracy 0.6335998773574829\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss 0.3392994701862335 Accurracy 0.6373999118804932\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss 0.3473966717720032 Accurracy 0.6339999437332153\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss 0.37062400579452515 Accurracy 0.637199878692627\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss 0.3313884139060974 Accurracy 0.6379998922348022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, CIFAR-10 Batch 3:  Loss 0.31046271324157715 Accurracy 0.6349998712539673\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss 0.3179214894771576 Accurracy 0.6429999470710754\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss 0.3477545380592346 Accurracy 0.6381999254226685\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss 0.3601873219013214 Accurracy 0.6387999057769775\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss 0.33594226837158203 Accurracy 0.6395998597145081\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss 0.30078834295272827 Accurracy 0.6375998854637146\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss 0.3315427899360657 Accurracy 0.6419999003410339\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss 0.33226478099823 Accurracy 0.6413999199867249\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss 0.35845139622688293 Accurracy 0.642599880695343\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss 0.32610785961151123 Accurracy 0.6457998752593994\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss 0.2869068682193756 Accurracy 0.6373999118804932\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss 0.31457462906837463 Accurracy 0.6427999138832092\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss 0.3228641450405121 Accurracy 0.6379998922348022\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss 0.35326582193374634 Accurracy 0.641799807548523\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss 0.32966357469558716 Accurracy 0.6405999064445496\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss 0.276601642370224 Accurracy 0.6389999389648438\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss 0.30724868178367615 Accurracy 0.6351999044418335\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss 0.3132946789264679 Accurracy 0.6355999112129211\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss 0.33384233713150024 Accurracy 0.6409999132156372\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss 0.3112339973449707 Accurracy 0.6431999206542969\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss 0.2740131616592407 Accurracy 0.6355999112129211\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss 0.308204710483551 Accurracy 0.636199951171875\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss 0.3067418932914734 Accurracy 0.6319999098777771\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss 0.3250761330127716 Accurracy 0.6395999193191528\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss 0.30836498737335205 Accurracy 0.6387998461723328\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss 0.268854022026062 Accurracy 0.6299998760223389\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss 0.28679969906806946 Accurracy 0.6397998929023743\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss 0.3112664520740509 Accurracy 0.6373998522758484\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss 0.31459543108940125 Accurracy 0.637199878692627\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss 0.28930893540382385 Accurracy 0.6363999247550964\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss 0.259199321269989 Accurracy 0.6369999051094055\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss 0.2778564393520355 Accurracy 0.6401998996734619\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss 0.291862815618515 Accurracy 0.6367999315261841\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss 0.30606794357299805 Accurracy 0.6395999789237976\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss 0.28520897030830383 Accurracy 0.6383998990058899\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss 0.24581894278526306 Accurracy 0.6391998529434204\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss 0.26462602615356445 Accurracy 0.634399950504303\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss 0.28940171003341675 Accurracy 0.6383998990058899\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss 0.29605042934417725 Accurracy 0.6383998394012451\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss 0.27437731623649597 Accurracy 0.6411998867988586\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss 0.24996383488178253 Accurracy 0.6367999315261841\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss 0.2635446786880493 Accurracy 0.6405999064445496\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss 0.27517256140708923 Accurracy 0.6403998732566833\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss 0.2968541085720062 Accurracy 0.6361998319625854\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss 0.2764732241630554 Accurracy 0.6465998888015747\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss 0.24882172048091888 Accurracy 0.6357998847961426\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss 0.2648524343967438 Accurracy 0.6401998996734619\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss 0.2718208134174347 Accurracy 0.6349998712539673\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss 0.29779788851737976 Accurracy 0.6359999179840088\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss 0.2707620859146118 Accurracy 0.6369999051094055\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss 0.23852193355560303 Accurracy 0.6355998516082764\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss 0.2537514567375183 Accurracy 0.6393998861312866\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss 0.2652629613876343 Accurracy 0.6365998983383179\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss 0.284747451543808 Accurracy 0.638999879360199\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss 0.2543928623199463 Accurracy 0.6387999057769775\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss 0.23485353589057922 Accurracy 0.6339998245239258\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss 0.23547418415546417 Accurracy 0.6377999186515808\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss 0.26193496584892273 Accurracy 0.6417999267578125\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss 0.2764608860015869 Accurracy 0.6373999118804932\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss 0.2520776093006134 Accurracy 0.6375998854637146\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss 0.22187256813049316 Accurracy 0.6387999057769775\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss 0.23835384845733643 Accurracy 0.640799880027771\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss 0.2642397880554199 Accurracy 0.640799880027771\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss 0.25832465291023254 Accurracy 0.6413999199867249\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss 0.2467774748802185 Accurracy 0.6399998664855957\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss 0.21881340444087982 Accurracy 0.6383998394012451\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss 0.22778275609016418 Accurracy 0.6441998481750488\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss 0.2477579116821289 Accurracy 0.6361998915672302\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss 0.26281559467315674 Accurracy 0.6429998874664307\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss 0.2447015941143036 Accurracy 0.638999879360199\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss 0.20835205912590027 Accurracy 0.6321998834609985\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss 0.22771093249320984 Accurracy 0.6411998867988586\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss 0.23902764916419983 Accurracy 0.6403998732566833\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss 0.2575858235359192 Accurracy 0.6395999193191528\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss 0.22613678872585297 Accurracy 0.6447998881340027\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss 0.2086622267961502 Accurracy 0.6371999382972717\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss 0.22950321435928345 Accurracy 0.643799901008606\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss 0.2455429583787918 Accurracy 0.6411999464035034\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss 0.24960599839687347 Accurracy 0.6403999328613281\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss 0.23311690986156464 Accurracy 0.6433998942375183\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss 0.19651567935943604 Accurracy 0.6399998664855957\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss 0.21144536137580872 Accurracy 0.640799880027771\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss 0.23046807944774628 Accurracy 0.6365998983383179\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss 0.2441442310810089 Accurracy 0.6379998922348022\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss 0.22090618312358856 Accurracy 0.6411998867988586\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss 0.19286739826202393 Accurracy 0.6331998705863953\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss 0.2195906788110733 Accurracy 0.6407999396324158\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss 0.21875299513339996 Accurracy 0.6351999044418335\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss 0.2368682473897934 Accurracy 0.6373999118804932\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss 0.21628867089748383 Accurracy 0.6373999118804932\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss 0.1934480369091034 Accurracy 0.6351999044418335\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss 0.20337054133415222 Accurracy 0.6469998955726624\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss 0.21757923066616058 Accurracy 0.6411998867988586\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss 0.2257642149925232 Accurracy 0.6375999450683594\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss 0.21877700090408325 Accurracy 0.637199878692627\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss 0.18408164381980896 Accurracy 0.6377999186515808\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss 0.21208235621452332 Accurracy 0.6395998597145081\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss 0.21536242961883545 Accurracy 0.6389999389648438\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss 0.22266361117362976 Accurracy 0.6385999321937561\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss 0.19698195159435272 Accurracy 0.6453999280929565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, CIFAR-10 Batch 3:  Loss 0.17497940361499786 Accurracy 0.6381999254226685\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss 0.2019716203212738 Accurracy 0.6403998732566833\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss 0.20949740707874298 Accurracy 0.6369999051094055\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss 0.22254890203475952 Accurracy 0.6395999193191528\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss 0.19374100863933563 Accurracy 0.6347998380661011\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss 0.16582107543945312 Accurracy 0.634199857711792\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss 0.1894213855266571 Accurracy 0.6403998136520386\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss 0.20263400673866272 Accurracy 0.6363998055458069\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss 0.210980623960495 Accurracy 0.6335998773574829\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss 0.19225411117076874 Accurracy 0.6387999057769775\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss 0.17045855522155762 Accurracy 0.6345998644828796\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss 0.18237553536891937 Accurracy 0.6435998678207397\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss 0.202505961060524 Accurracy 0.6409998536109924\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss 0.2089022696018219 Accurracy 0.6395999193191528\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss 0.19387631118297577 Accurracy 0.6407999396324158\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss 0.1747124195098877 Accurracy 0.6373999118804932\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss 0.18443253636360168 Accurracy 0.6385999321937561\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss 0.19573982059955597 Accurracy 0.6403998732566833\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss 0.20797505974769592 Accurracy 0.638999879360199\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss 0.18318183720111847 Accurracy 0.6373998522758484\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss 0.16279920935630798 Accurracy 0.6337999105453491\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss 0.1773461103439331 Accurracy 0.6401998996734619\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss 0.19211530685424805 Accurracy 0.6375999450683594\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss 0.19120031595230103 Accurracy 0.6327998638153076\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss 0.18414674699306488 Accurracy 0.6369999051094055\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss 0.1568625420331955 Accurracy 0.635999858379364\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss 0.17868797481060028 Accurracy 0.6479999423027039\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss 0.18474219739437103 Accurracy 0.6337998509407043\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss 0.19255049526691437 Accurracy 0.6379998922348022\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss 0.173366978764534 Accurracy 0.6373999118804932\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss 0.1594688892364502 Accurracy 0.6347998976707458\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss 0.17463219165802002 Accurracy 0.6433998942375183\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss 0.18071788549423218 Accurracy 0.6363999247550964\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss 0.18735960125923157 Accurracy 0.6417998671531677\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss 0.18166117370128632 Accurracy 0.6345999240875244\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss 0.14953938126564026 Accurracy 0.6353998780250549\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss 0.1677728295326233 Accurracy 0.6381999254226685\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss 0.17677363753318787 Accurracy 0.6355999112129211\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss 0.17924124002456665 Accurracy 0.6417999267578125\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss 0.17409023642539978 Accurracy 0.6409998536109924\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss 0.1515846699476242 Accurracy 0.6303999423980713\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss 0.15380576252937317 Accurracy 0.6363998651504517\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss 0.17210932075977325 Accurracy 0.6385999321937561\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss 0.18137283623218536 Accurracy 0.6359999179840088\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss 0.16542969644069672 Accurracy 0.6377999186515808\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss 0.1462675780057907 Accurracy 0.6331998705863953\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss 0.16274958848953247 Accurracy 0.6343998908996582\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss 0.1618436574935913 Accurracy 0.6387999057769775\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss 0.1683410108089447 Accurracy 0.6403998732566833\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss 0.16995880007743835 Accurracy 0.6405999064445496\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss 0.14817625284194946 Accurracy 0.6299999356269836\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss 0.15771496295928955 Accurracy 0.6355999112129211\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss 0.16521786153316498 Accurracy 0.6323999166488647\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss 0.16492007672786713 Accurracy 0.6391999125480652\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss 0.16248436272144318 Accurracy 0.638999879360199\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss 0.13419832289218903 Accurracy 0.6337999105453491\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss 0.14888498187065125 Accurracy 0.6367999315261841\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss 0.15925540030002594 Accurracy 0.6375998258590698\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss 0.1731792539358139 Accurracy 0.6401998400688171\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss 0.1570504903793335 Accurracy 0.643799901008606\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss 0.13508012890815735 Accurracy 0.6369999647140503\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss 0.15391477942466736 Accurracy 0.6423999071121216\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss 0.15733706951141357 Accurracy 0.6363998651504517\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss 0.16018235683441162 Accurracy 0.6381998658180237\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss 0.153495654463768 Accurracy 0.6417999267578125\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss 0.12885808944702148 Accurracy 0.6319999098777771\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss 0.14269094169139862 Accurracy 0.6409999132156372\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss 0.1492842137813568 Accurracy 0.6345999240875244\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss 0.15403619408607483 Accurracy 0.6413999199867249\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss 0.14565885066986084 Accurracy 0.6415998935699463\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss 0.12699909508228302 Accurracy 0.6335998773574829\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss 0.14625820517539978 Accurracy 0.6347998380661011\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss 0.14685210585594177 Accurracy 0.6289998888969421\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss 0.14713267982006073 Accurracy 0.6379998922348022\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss 0.14251910150051117 Accurracy 0.6385999321937561\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss 0.1239272952079773 Accurracy 0.6387998461723328\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss 0.13966041803359985 Accurracy 0.637199878692627\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss 0.13981285691261292 Accurracy 0.6311998963356018\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss 0.14469259977340698 Accurracy 0.6411999464035034\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss 0.14172175526618958 Accurracy 0.6377999186515808\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss 0.13044217228889465 Accurracy 0.6351999640464783\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss 0.1362212598323822 Accurracy 0.6421999335289001\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss 0.1510763317346573 Accurracy 0.6399999260902405\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss 0.14151345193386078 Accurracy 0.640799880027771\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss 0.13813477754592896 Accurracy 0.638999879360199\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss 0.12910132110118866 Accurracy 0.6379998922348022\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss 0.140703946352005 Accurracy 0.635999858379364\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss 0.143119215965271 Accurracy 0.6343998908996582\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss 0.13714194297790527 Accurracy 0.642599880695343\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss 0.13857579231262207 Accurracy 0.6343998908996582\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss 0.11953093856573105 Accurracy 0.6375999450683594\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss 0.1300588846206665 Accurracy 0.6399998664855957\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss 0.13259422779083252 Accurracy 0.6321998834609985\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss 0.15064188838005066 Accurracy 0.643799901008606\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss 0.13284894824028015 Accurracy 0.6377999186515808\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss 0.11571218073368073 Accurracy 0.6367999315261841\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss 0.12942248582839966 Accurracy 0.6377999782562256\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss 0.13689661026000977 Accurracy 0.6377999186515808\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6374655336141586\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3XmcZFV5//HP03v3LD0LMDOAMOwMgiKLKCgMcYmKihqF\naIyA0bjEDZfILyYGzS/GnzHuUWJcRg0KLlFjEEVREFFUZmQflG2A2Rlmpntmeu96fn+cc+vevl1d\nXT1d3dVd/X2/XvWqqnvuPfdUdXXVqaeec465OyIiIiIiAg21boCIiIiIyEyhzrGIiIiISKTOsYiI\niIhIpM6xiIiIiEikzrGIiIiISKTOsYiIiIhIpM6xiIiIiEikzrGIiIiISKTOsYiIiIhIpM6xiIiI\niEikzrGIiIiISKTOsYiIiIhIpM6xiIiIiEikzrGIiIiISKTOcY2Z2eFm9jIze5OZ/R8zu8zM3mpm\nrzCz08xsfq3bOBYzazCz883sKjO738y6zcwzl+/Vuo0iM42Zrcz9n1xejX1nKjNbnXsMF9e6TSIi\n5TTVugFzkZktAd4EvB44fJzdC2Z2D3ATcA1wvbv3TXETxxUfw7eBc2vdFpl+ZrYGuGic3YaA3cAO\nYB3hNfwNd++a2taJiIjsP0WOp5mZvRC4B/i/jN8xhvA3OpHQmf5f4OVT17oJ+SoT6BgrejQnNQEH\nAMcDrwI+B2wys8vNTF/MZ5Hc/+6aWrdHRGQq6QNqGpnZBcA3GP2lpBu4E9gK9AOLgcOAVSX2rTkz\nexpwXmbTw8AHgFuBPZntPdPZLpkV5gH/CJxtZs939/5aN0hERCRLneNpYmZHEaKt2c7uXcD7gB+6\n+1CJY+YD5wCvAF4KLJyGplbiZbn757v77TVpicwU7yGk2WQ1AcuAZwBvJnzhS5xLiCS/dlpaJyIi\nUiF1jqfPPwOtmfs/BV7s7r1jHeDuewl5xteY2VuB1xGiy7V2aub2BnWMBdjh7htKbL8fuNnMPg38\nF+FLXuJiM/uUu982HQ2cjeJzarVux2S4+w3M8scgInPLjPvJvh6ZWTvw4symQeCich3jPHff4+4f\nd/efVr2BE3dQ5vbmmrVCZg137wH+AvhjZrMBb6xNi0REREpT53h6nAK0Z+7/yt1nc6cyO73cYM1a\nIbNK/DL48dzmZ9WiLSIiImNRWsX0WJ67v2k6T25mC4FnAocASwmD5rYBv3H3R/anyio2ryrM7EhC\nusehQAuwAfi5u28f57hDCTmxTyA8ri3xuI2TaMshwBOBI4FFcfNO4BHg13N8KrPrc/ePMrNGdx+e\nSCVmdiJwArCCMMhvg7t/vYLjWoCnAysJv4AUgO3AHdVIDzKzY4CnAgcDfcBG4LfuPq3/8yXadSxw\nMnAg4TXZQ3it3wXc4+6FGjZvXGb2BOBphBz2BYT/p83ATe6+u8rnOpIQ0HgC0Eh4r7zZ3R+cRJ3H\nEZ7/5YTgwhCwF3gUuA+41919kk0XkWpxd12m+AL8OeCZy7XTdN7TgGuBgdz5s5c7CNNsWZl6Vpc5\nfqzLDfHYDft7bK4Na7L7ZLafA/yc0MnJ1zMAfBaYX6K+E4AfjnFcAfgOcEiFz3NDbMfngAfGeWzD\nwE+Acyus+yu54z8/gb//v+SO/UG5v/MEX1trcnVfXOFx7SWek4NK7Jd93dyQ2X4JoUOXr2P3OOc9\nDvg64YvhWH+bjcA7gZb9eD7OAn4zRr1DhLEDp8Z9V+bKLy9Tb8X7ljh2EfBPhC9l5V6TjwFfAk4f\n529c0aWC94+KXivx2AuA28qcbzD+Pz1tAnXekDl+Q2b7GYQvb6XeExy4BXj6BM7TDLyLkHc/3vO2\nm/Ce85xq/H/qoosuk7vUvAFz4QL8Se6NcA+waArPZ8BHyrzJl7rcACweo778h1tF9cVjN+zvsbk2\njPigjtveVuFj/B2ZDjJhto2eCo7bADyhguf7tfvxGB34N6BxnLrnAffmjruwgjY9N/fcbASWVvE1\ntibXposrPG6/OseEwazfLPNcluwcE/4XPkjoRFX6d7mrkr975hx/V+HrcICQd70yt/3yMnVXvG/u\nuJcCuyb4erxtnL9xRZcK3j/Gfa0QZub56QTP/QmgoYK6b8gcsyFueyvlgwjZv+EFFZzjQMLCNxN9\n/r5Xrf9RXXTRZf8vSquYHmsJEcPGeH8+8FUze5WHGSmq7T+Bv8ptGyBEPjYTIkqnERZoSJwD/MLM\nznb3XVPQpqqKc0Z/Mt51QnTpAUJn6GTgqMzupwGfBi4xs3OBq0lTiu6NlwHCvNInZY47nMoWO8nn\n7vcCdxN+tu4mdAgPA55ESPlIvJPQabtsrIrdfV98rL8B2uLmz5vZre7+QKljzGw58DXS9Jdh4FXu\n/vg4j2M6HJK770Al7foEYUrD5Jjfk3agjwSOyB9gZkaIvP9lrqiX0HFJ8v6PJrxmkufricCvzOx0\ndy87O4yZvYMwE03WMOHv9SghBeAphPSPZkKHM/+/WVWxTR9jdPrTVsIvRTuADkIK0kmMnEWn5sxs\nAXAj4W+StQv4bbxeQUizyLb97YT3tFdP8HyvBj6V2XQXIdrbT3gfOZX0uWwG1pjZ7939vjHqM+C/\nCX/3rG2E+ex3EL5Mdcb6j0YpjiIzS61753PlQljdLh8l2ExYEOEkqvdz90W5cxQIHYtFuf2aCB/S\nXbn9v1GizjZCBCu5bMzsf0uuLLksj8ceGu/nU0vePcZxxWNzbViTOz6Jiv0vcFSJ/S8gdIKyz8PT\n43PuwK+Ak0sct5rQWcue6wXjPOfJFHv/Es9RMhpM+FLyXmBfrl1nVPB3fWOuTbdS4ud/Qkc9H3H7\nhyl4Pef/HhdXeNxf5467f4z9NmT2yaZCfA04tMT+K0tsuyx3rp3xeWwrse8RwPdz+/+Y8ulGJzE6\n2vj1/Os3/k0uIOQ2J+3IHnN5mXOsrHTfuP+fEjrn2WNuBM4s9VgIncsXEX7SX5srO4D0fzJb37cZ\n+3+31N9h9UReK8CXc/t3A28AmnP7dRJ+fclH7d8wTv03ZPbdS/o+8V3g6BL7rwJuz53j6jL1n5fb\n9z7CwNOSryXCr0PnA1cB36r2/6ouuugy8UvNGzBXLoQoSF/uTTN7eZyQl/gPwHOAeftxjvmE3LVs\nvZeOc8wZjOysOePkvTFGPug4x0zoA7LE8WtKPGdXUuZnVMKS26U61D8FWssc98JKPwjj/svL1Vdi\n/6fnXgtl688cl08r+GSJfd6X2+f6cs/RJF7P+b/HuH9Pwpes9bnjSuZQUzod518m0L4nMjKV4lFK\ndNxyxxgh9zZ7zvPK7P/z3L6fqaBN+Y5x1TrHhGjwtnybKv37A8vKlGXrXDPB10rF//uEgcPZfXuA\ns8ap/y25Y/YyRopY3P+GEn+Dz1D+i9AyRqap9I11DsLYg2S/QeCICTxXo7646aKLLtN/0VRu08TD\nQgd/SXhTLWUJ8AJCfuR1wC4zu8nM3hBnm6jERYRoSuJH7p6fOivfrt8A789tfnuF56ulzYQIUblR\n9l8kRMYTySj9v/Qyyxa7+/8Cf8hsWl2uIe6+tVx9Jfb/NfDvmU0vMbNKftp+HZAdMf82Mzs/uWNm\nzyAs4514DHj1OM/RtDCzNkLU9/hc0X9UWMVtwN9P4JR/S/pTtQOv8NKLlBS5uxNW8svOVFLyf8HM\nnsjI18UfCWky5eq/O7ZrqryekXOQ/xx4a6V/f3ffNiWtmpi35e5/wN1vLneAu3+G8AtSYh4TS125\nixBE8DLn2Ebo9CZaCWkdpWRXgrzN3R+qtCHuPtbng4hMI3WOp5G7f4vw8+YvK9i9mTDF2BXAg2b2\n5pjLVs5f5O7/Y4VN+xShI5V4gZktqfDYWvm8j5Ov7e4DQP6D9Sp331JB/T/L3D4o5vFW0/czt1sY\nnV85irt3AxcSfspPfNnMDjOzpcA3SPPaHXhNhY+1Gg4ws5W5y9FmdqaZ/S1wD/Dy3DFXuvvaCuv/\nhFc43ZuZLQJemdl0jbvfUsmxsXPy+cymc82so8Su+f+1j8TX23i+xNRN5fj63P2yHb6ZxszmAS/J\nbNpFSAmrRP6L00Tyjj/u7pXM1/7D3P0nV3DMgRNoh4jMEOocTzN3/727PxM4mxDZLDsPb7SUEGm8\nKs7TOkqMPGaXdX7Q3X9bYZsGgW9lq2PsqMhMcV2F++UHrf2kwuPuz92f8IecBQvM7OB8x5HRg6Xy\nEdWS3P1WQt5yYjGhU7yGkN+d+Fd3/9FE2zwJ/wo8lLvcR/hy8v8YPWDuZkZ35sr5wQT2PYvw5TLx\n7QkcC3BT5nYTIfUo7+mZ28nUf+OKUdxvjbvjBJnZgYS0jcTvfPYt6346IwemfbfSX2TiY70ns+mk\nOLCvEpX+n9ybuz/We0L2V6fDzexvKqxfRGYIjZCtEXe/ifghbGYnECLKpxE+IE6m9BeXCwgjnUu9\n2Z7IyJkQfjPBJt1C+Ek5cSqjIyUzSf6Daizduft/KLnX+MeNm9piZo3AswmzKpxO6PCW/DJTwuIK\n98PdPxFn3UiWJD8zt8sthNzjmaiXMMvI+yuM1gE84u47J3COs3L3H49fSCrVmLtf6thTMrfv84kt\nRPG7CexbqXwH/qaSe81sp+bu78972AnxdgPhfXS856HbK1+tNL94z1jvCVcBl2buf8bMXkIYaHit\nz4LZgETmOnWOZwB3v4cQ9fgCFH8WfgnhDfZJud3fbGZfdPd1ue35KEbJaYbKyHcaZ/rPgZWuMjdU\npeOaS+4VmdnTCfmzJ5Xbr4xK88oTlxCmMzsst3038Ep3z7e/FoYJz/fjhLbeBHx9gh1dGJnyU4lD\nc/cnEnUuZUSKUcyfzv69Sk6pV0b+V4lqyKf9rJ+Cc0y1WryHVbxapbsP5jLbSr4nuPtvzeyzjAw2\nPDteCmZ2J+GXk19QwSqeIjL9lFYxA7n7bndfQ4h8fLDELvlBK5AuU5zIRz7Hk/+QqDiSWQuTGGRW\n9cFpZvY8wuCn/e0YwwT/F2MH80Mlit413sCzKXKJu1vu0uTuS939WHe/0N0/sx8dYwizD0xEtfPl\n5+fuV/t/rRqW5u5XdUnlaVKL97CpGqz6FsKvNz257Q2EXOU3EyLMW8zs52b28grGlIjINFHneAbz\n4B8Ji1ZkPbsW7ZHR4sDF/2LkYgQbCMv2Pp+wbPEiwhRNxY4jJRatmOB5lxKm/ct7tZnN9f/rslH+\n/TAbOy2zZiBePYrv3R8iLFDzXuDXjP41CsJn8GpCHvqNZrZi2hopImNSWsXs8GnCLAWJQ8ys3d17\nM9vykaKJ/kzfmbuvvLjKvJmRUburgIsqmLmg0sFCo2RWfsuvNgdhNb+/p/QvDnNFPjp9grtXM82g\n2v9r1ZB/zPko7GxQd+9hcQq4jwAfMbP5wFMJczmfS8iNz34GPxP4kZk9dSJTQ4pI9c31CNNsUWrU\nef4nw3xe5tETPMex49QnpZ2Xud0FvK7CKb0mMzXcpbnz/paRs56838yeOYn6Z7t8DucBJffaT3G6\nt+xP/keNte8YJvq/WYn8MterpuAcU62u38Pcfa+7/8zdP+DuqwlLYP89YZBq4knAa2vRPhFJqXM8\nO5TKi8vn493FyPlvnzrBc+Snbqt0/tlK1evPvNkP8F+6+74Kj9uvqfLM7HTgw5lNuwizY7yG9Dlu\nBL4eUy/movycxqWmYpus7IDYY+Ig2kqdXu3GMPoxz8YvR/n3nIn+3bL/UwXCwjEzlrvvcPd/ZvSU\nhi+qRXtEJKXO8exwXO7+3vwCGPFnuOyHy9Fmlp8aqSQzayJ0sIrVMfFplMaT/5mw0inOZrrsT7kV\nDSCKaRGvmuiJ4kqJVzEyp/a17v6Iu/+YMNdw4lDC1FFz0c8Y+WXsgik4x68ztxuAP6vkoJgP/opx\nd5wgd3+M8AU58VQzm8wA0bzs/+9U/e/+jpF5uS8da173PDN7EiPneb7L3fdUs3FT6GpGPr8ra9QO\nEYnUOZ4GZrbMzJZNoor8z2w3jLHf13P388tCj+UtjFx29lp3f7zCYyuVH0le7RXnaiWbJ5n/WXcs\nf0mFi37k/CdhgE/i0+7+vcz99zHyS82LzGw2LAVeVTHPM/u8nG5m1e6QXpm7/7cVduReS+lc8Wr4\nfO7+x6o4A0L2/3dK/nfjry7ZlSOXUHpO91LyOfb/VZVGTYM47WL2F6dK0rJEZAqpczw9VhGWgP6w\nmR007t4ZZvZnwJtym/OzVyS+wsgPsReb2ZvH2Dep/3TCzApZn5pIGyv0ICOjQudOwTlq4c7M7VPN\n7JxyO5vZUwkDLCfEzP6akRHQ3wPvye4TP2T/nJGvgY+YWXbBirnig4xMR/rSeH+bPDNbYWYvKFXm\n7ncDN2Y2HQt8bJz6TiAMzpoqXwS2Ze4/G/h4pR3kcb7AZ+cQPj0OLpsK+feef4rvUWMyszcB52c2\n7SM8FzVhZm+KKxZWuv/zGTn9YKULFYnIFFHnePp0EKb02Whm3zWzPyv3Bmpmq8zs88A3Gbli1zpG\nR4gBiD8jvjO3+dNm9q9mNmIkt5k1mdklhOWUsx9034w/0VdVTPvIRjVXm9kXzOxZZnZMbnnl2RRV\nzi9N/B0ze3F+JzNrN7NLgesJo/B3VHoCMzsR+ERm017gwlIj2uMcx6/LbGohLDs+VZ2ZGcndbyMM\ndkrMB643s0+Z2ZgD6MxskZldYGZXE6bke02Z07wVyK7y9zdmdmX+9WtmDTFyfQNhIO2UzEHs7j2E\n9ma/FLyd8LifXuoYM2s1sxea2XcovyLmLzK35wPXmNlL4/tUfmn0yTyGXwBfy2yaB/zEzP4qpn9l\n277QzD4CfCZXzXv2cz7tankv8Eh8LbxkrGWs43vwawjLv2fNmqi3SL3SVG7Tr5mw+t1LAMzsfuAR\nQmepQPjwPAF4QoljNwKvKLcAhrt/yczOBi6KmxqAdwNvNbNfA1sI0zydzuhR/PcwOkpdTZ9m5NK+\nfxUveTcS5v6cDb5EmD3imHh/KfB9M3uY8EWmj/Az9BmEL0gQRqe/iTC3aVlm1kH4paA9s/mN7j7m\n6mHu/m0zuwJ4Y9x0DHAF8OoKH1NdcPd/iZ21v46bGgkd2rea2UOEJch3Ef4nFxGep5UTqP9OM3sv\nIyPGrwIuNLNbgEcJHclTCTMTQPj15FKmKB/c3a8zs3cD/0Y6P/O5wK/MbAtwB2HFwnZCXvqTSOfo\nLjUrTuILwLuAtnj/7HgpZbKpHG8hLJSRrA7aGc///8zst4QvF8uBp2fak7jK3T83yfNXQxvhtfAq\nwM3sj8BDpNPLrQCewujp577n7pNd0VFEJkmd4+mxk9D5LTWl1NFUNmXRT4HXV7j62SXxnO8g/aBq\npXyH85fA+VMZcXH3q83sDELnoC64e3+MFP+MtAMEcHi85O0lDMi6t8JTfJrwZSnxZXfP57uWcinh\ni0gyKOsvzOx6d59Tg/Tc/Q1mdgdhsGL2C8YRVLYQS9m5ct394/ELzD+R/q81MvJLYGKI8GXwFyXK\nqia2aROhQ5mNWq5g5Gt0InVuMLOLCZ369nF2nxR3744pMP/NyPSrpYSFdcby75RePbTWjDCoOj+w\nOu9q0qCGiNSQ0iqmgbvfQYh0/AkhynQrMFzBoX2ED4gXuvtzKl0WOK7O9E7C1EbXUXplpsTdhJ9i\nz56OnyJju84gfJD9jhDFmtUDUNz9XuAUws+hYz3Xe4GvAk9y9x9VUq+ZvZKRgzHvJUQ+K2lTH2Hh\nmOzytZ82s/0ZCDirufu/EzrCHwU2VXDIHwk/1Z/p7uP+khKn4zqbMN90KQXC/+FZ7v7Viho9Se7+\nTcLgzY8yMg+5lG2EwXxlO2bufjVh/MQHCCkiWxg5R2/VuPtu4FmEyOsdZXYdJqQqneXub5nEsvLV\ndD7hObqFkWk3pRQI7T/P3f9ci3+IzAzmXq/Tz85sMdp0bLwcRBrh6SZEfe8G7omDrCZ7rk7Ch/ch\nhIEfewkfiL+ptMMtlYlzC59NiBq3E57nTcBNMSdUaix+QXgy4ZecRYRptHYDDxD+58brTJar+xjC\nl9IVhC+3m4Dfuvujk233JNpkhMf7ROBAQqrH3ti2u4H1PsM/CMzsMMLzuozwXrkT2Ez4v6r5Snhj\nMbM24ETCr4PLCc/9IGHQ7P3AuhrnR4tICeoci4iIiIhESqsQEREREYnUORYRERERidQ5FhERERGJ\n1DkWEREREYnUORYRERERidQ5FhERERGJ1DkWEREREYnUORYRERERidQ5FhERERGJ1DkWEREREYnU\nORYRERERidQ5FhERERGJ1DkWEREREYnUORYRERERidQ5FhERERGJ1DkWEREREYnUORYRERERidQ5\nFhERERGJ1DkWEREREYnUORYRERERidQ5FhERERGJ1DkWEREREYnUORYRERERidQ5FhERERGJmmrd\nACnNzC4GVgLfc/fbatsaERERkblBneOZ62LgHGADoM6xiIiIyDRQWoWIiIiISKTOsYiIiIhIpM7x\nfjCzVWZ2hZn90cx6zGy3md1pZp8ys1Mz+7Wa2SvM7KtmdruZ7TCzPjN72MyuzO6bOeZiM3NCSgXA\nl83MM5cN0/QwRUREROYcc/dat2FWMbO3Ah8HGuOmfcAgsCjev9HdV8d9Xwj8IG53YDfQDrTFbUPA\na939a5n6LwQ+CSwBmoFuoDfThEfd/fTqPioRERERAUWOJ8TMXgF8itAx/jZwgrvPd/fFwFLg1cDa\nzCF74/5nA/PdfYm7twOHA58gDIj8vJkdlhzg7le7+3LgV3HT2919eeaijrGIiIjIFFHkuEJm1gw8\nBBwCfMPdX1WFOr8IvBa43N0/kCu7gZBacYm7r5nsuURERERkfIocV+5ZhI7xMPCeKtWZpFycVaX6\nRERERGQSNM9x5Z4Wr293902VHmRmS4C/AZ4PHAd0kuYrJw6uSgtFREREZFLUOa7csnj9SKUHmNkJ\nwM8yxwLsIQywc6AFWAzMq1IbRURERGQSlFYxtb5M6BivA54HLHD3he6+LA66e0Xcz2rVQBERERFJ\nKXJcuW3x+vBKdo4zUDyVkKP84jFSMZaV2CYiIiIiNaLIceVuiddPMrNDKtj/0Hj9WJkc5WeXOb4Q\nrxVVFhEREZkm6hxX7npgE2Ew3b9WsH9XvF5mZgflC83sJKDcdHDd8XpRmX1EREREpIrUOa6Quw8C\n74p3X2lm3zSz45NyM1tiZq83s0/FTeuBjYTI79VmdnTcr9nMXgb8hLBIyFjujtcvM7POaj4WERER\nESlNi4BMkJm9kxA5Tr5Y7CUsA11q+eiXElbSS/bdA7QSZql4BHgf8DXgYXdfmTvP8cDtcd8hYDth\nmeqN7v6MKXhoIiIiInOeIscT5O4fA55CmIliA9BMmJbtDuCTwKWZfb8L/AkhSrwn7vsw8NFYx8Yy\n57kXeA7wI0KKxnLCYMBDxzpGRERERCZHkWMRERERkUiRYxERERGRSJ1jEREREZFInWMRERERkUid\nYxERERGRSJ1jEREREZFInWMRERERkUidYxERERGRSJ1jEREREZFInWMRERERkaip1g0QEalHZvYQ\nsJCwzLyIiEzcSqDb3Y+YzpPWbef46KOOGHNd7IaGsQPmxdW0Ld2nf2AAgJ6eHgAKw8PpAWYAtDQ3\nA9Dc2FgsOuiApQAcd8yxoW7SJj24YQMA23Y8VtzW19cHwFBSf+YRWGxPUodhox5Pa2vriGuA4aFQ\nV29se3a58ObY5kc3bUwrE5FqWdje3r5k1apVS2rdEBGR2Wj9+vX09vZO+3nrtnNM7DxamW5fuU4y\n2U5kQ6ikvSV0Jvv6hoplTU2hM7yoc2G87iyWHbAkfCbOnz8fgMGh9DiLDSsUCsVtSae2EDvJnu3e\n+8jrgqfH5esaznTeGxsbYjubRpW5j/n9QaQmzGwl8BDwFXe/uIL9Lwa+DFzi7muq1IbVwM+BD7j7\n5ZOoasOqVauWrF27thrNEhGZc0499VTWrVu3YbrPq5xjEREREZGojiPHIjIHfBe4BdhS64aUctem\nLlZedk2tmyF1bsOHz6t1E0TqSt12jpN0ilKpA0kaQjbFINnWEtMkGrL5GHE3awqB9qFsvL0QUiX2\ndu8CYLAvzY2xQjj30GDYJ8ldBti3b184vDA6PaKtLaRXDAykaRiDA4OhXTEVJPu4ktvJ4xmRVpHL\nR07ymkXqgbt3AV21boeIiNQPpVWIyIxkZseb2ffMbKeZ7TOzX5rZc3P7XGxmHnOPs9s3xMtCM/tY\nvD1oZpdn9llmZl80s21m1mtmt5nZRdPz6EREZKaq28hxJSwTHc5Hkxub0lknOjraAejpCdHe9ra2\nYlkyg0Uyeq65OX1KkxGWSZR4aDgTCY7nydaVDNhrjHW0tKRl+/aOjDSXihwXhuOAvMzAv8FkYGKJ\nx5wM0hOZgY4Afg3cCfwHsAK4ELjWzF7l7ldXUEcL8DNgCXAd0E0Y7IeZHQD8CjgS+GW8rACuiPuK\niMgcpd6RiMxEZwMfdff3JBvM7DOEDvMVZnatu3ePU8cK4B7gHHfflyv7EKFj/Al3v7TEOSpmZmNN\nR3H8ROoREZGZYU53jhszcxInubwNFrc1phknTS3haVoyP0Ryu7p2F8vaOsK2gw5cBsBhh60sliVz\nH3d3h5TILVu2Fsu69uyJJ07b0BKjwha3DQ6mEeB58xbEWyFK3J/JHR4q5jLHCPLQYLGsQKizOT6G\nhQvai2UL4/RzIjNQF/DB7AZ3v9XMrgQuAl4KfKWCet6V7xibWTPwF8Ae4PIy5xARkTlIOcciMhOt\nc/c9JbbfEK+fUkEdfcAdJbYfD3QAt8UBfWOdoyLufmqpC3DvROoREZGZQZ1jEZmJto2xPfn5pXOM\n8qztXnqlm+TY8c4hIiJzUN2mVZT6TEy2DQ4OjrgGaGwMT0VTY1wGmuZiWZJ+sHx5WA56+YplxbKD\nVxwKwIFLl4d9F6Sf2Q8//CAAt/7ud+F8Q+lUbsXV+TKr9DU2hXMOxHSK4eH0MVhM9/BCGMg3r6Mj\nfVzJynpJ/ZYe1xgH4HW0h32yK/gtWrwYkRlq2Rjbl8frSqZvG2sJyOTY8c4hIiJzUN12jkVkVjvF\nzBaUSK2FicvgAAAgAElEQVRYHa9/P4m67wV6gJPNrLNEasXq0YfsnxMP6WStFmgQEZlV6rZznExZ\nVm4RkOwCHIODIeo6PBgGui3oWFQsO/bIwwA486wzATjttKcWyzoXLwGgrz9Eoe+8I01xvPvuuwHY\nuHFjON9wer6WlpZwvkzzurtDP2CokCzqkZZ50laPU7k1pwP5WuOUbI3NLbHu9M/aOW8eAAvbw0C8\nee2ZAXnzNCBPZqxO4P1AdraK0wgD6boIK+PtF3cfjIPuXk8YkJedrSI5h4iIzFF12zkWkVntF8Dr\nzOwM4GbSeY4bgDdUMI3beP4OeBbwjtghTuY5vhD4IfDiSdYvIiKzlAbkichM9BBwJrALeCNwAbAO\neEGFC4CU5e47gLOALxNmr3gHcDLwJuDjk61fRERmr7qNHCcpEw2ZAW/5dIrsPMcLFswH4IBFIQ3h\nuKMOL5YNx5SLo1YeCcBBB6TjeJKV7pKBcl3d6RzIAwPpADyAgmfSOGIaxr7e/nTbUKgjmefYLPvd\nJa7gF+vo68sMJmwPcy13xOvmpvS49taw7eDlYYzRimXpWKPOzkoG/ItMH3ffQLqgI8D54+y/BlhT\nYvvKCs61FXjtGMU2xnYREalzihyLiIiIiER1GzlujqvTZQfdJWPzmuIAtrbM4LTnPvc5ABxzeIgK\nP7bxwWJZW0cYuLZwfoguDw+lI+UsrqTXHKdTG86cb9euXQB0dYXB8AMDabR3INaRHTDY0tIaKw1B\nq6GhtCyp1+Ngveam9E+3YH5YPW9RZ7je17O3WNbT1xtuxAh62/x0CrjOpZrKTURERCRLkWMRERER\nkahuI8eLFoQo6lAhjfL29PWEbXHxj+OOObJY9oynhenZDj/kIAD6jj+mWDZ/foiwzmsLdfpQep5k\nkY1djz0GwOZHHiqWDQ6EqdnaWsM+yUIjAM2FcHtgMI00x6Aw1hDKLHuiGEW2xlBXUyaXevHiMO3c\nYU8IC5IkudUAzQ3h9gGLwj7LDz64WDZv3nxEREREJKXIsYiIiIhIpM6xiIiIiEhUt2kVg/1hpbvW\njrbitnnzQnpES5zq7MRMWkW7h/SLnq4wmK2lLV09zknqCIP8+nr6imW7d+8E4Oc3/BSAO9etK5Yd\nuuKA0IaWkBJR8My0cg1hNbsdj6cr13Z17yueEaA5s9LdQGzfYEzDGBhIp4Db+Oijoe37wvGLF6Wr\n+zXEqd/mtYTH0GDp9HWbHt2IiIiIiKQUORYRERERieo2ctzcEiKzxVFuwKHLVwDwnHPOAeDgxUuL\nZYO7wuC5XTEqvOCgg4pl7fNCFHk4Du4bGs5M5RYHxh13/PHh+K4dxbKt20Jkdn5HWGyjpT2dRm1f\nT4j8FoZ3jWq7k4zMSwfWNcWp6ZJtDY3p95qBOMBwy9at4XrLlmLZori4yYpl4fHcc+/6YtnmTZtH\nnVtERERkLlPkWEREREQkqtvIcce8sAz0wgXpdGUnP+nJAJx4whMBKOzpKZb1DIdo7XDM8+2Yl0Z5\n5y+MU7gVwneJ1tZ08ZClBy4BYHmcAm75wWnEee3aWwH4wx/+CMCdd99dLNu4OUR3hzOR7f7BZJGQ\nEB1ubm4plhWS9sWodXYp6oaYQ70wTl/X0pIed8jysKhJ/1CYFu6mm29O68wsWCIiIiIiihyLiIiI\niBSpcywiIiIiEtVtWkVjHCg3YoW8gTDYbnucfq2VdMDbAYeE9IPOAw8MG9pai2Uxm6I4tm/Q0zqb\nLTyFHleuW37wIcWyp8bp2nr2hXSJjZu2Fcv27gtt6drTXdyWDPQbjCkQff3pdG2FQi6tokRKRP/Q\nYGx6On3dQJzSrhCnfuvZl6aSDBbTOEREREQEFDkWkVnCzG4wMx9/zxHHuJndMEVNEhGROlS3keP+\nvrhQR6b7f+c99wDQMT8M1jv2yHQREOsLEdWGGF1ubUoXy2iM06Y1FKPRQ8WyfX0hktvcEvYf9vSz\nuylGcJceGKLSBy5bXizr2rs3Hp8uKGIxkptUkUSQARoaY3viVG7JFHJBOGBgIBw/NJRGtnvjwiDN\nydRvmfaltYuIiIgI1HHnWEQEWAX0jLvXFLlrUxcrL7umVqefEzZ8+LxaN0FE6ow6xyJSt9z93lq3\nQUREZpe67RwPDIZ0hf7dvcVt3V27AXhCHDR3UpzvGMDinMK7u0MawrxCOlivvS2kIjS3hOsCaWrC\nwHDYb6AQ0h7M0nSHlpi+ceIpTwGgrXNBsWzh0rA633333VfctmlTWFFv9+7Qzt7edEBeMjivsSH8\nyRob07SPZJAeHhIlsoP1hmNbC0PhuiGTjtHUls7lLFJLZvZi4O3ACcAS4HHgPuBqd/9sbt8m4G+B\nS4DDgO3A14F/cPeB3L4O3OjuqzPbLgf+ETgXOBx4B3A8sAf4X+Dv3H1r1R+kiIjMCnXbORaR2cHM\n/hr4D2Ar8ANgB3AQ8CRCB/izuUO+DjwTuBboBl5A6CwfFPev1KXAc4GrgR8Bz4jHrzazM9z9sQrb\nv3aMouMn0BYREZkh6rZzvDcOeGtqSh9iU4y2btoYIrSb4yp1AMcccwyQRlb7etOBcoNxoFtTc6wr\nE331hmQ1u2YAWtvSadSaGkM0uq09bDvuuOOKZfPnh5X7WlvTKeP6Y3S4oSG0s6cnjXrv2hWiyRYH\n5GUfVzIlW1JWjCRnbnsciDecjSr3jwiyidTKG4AB4Mnuvj1bYGYHlNj/KOCJ7r4z7vM+4HbgNWb2\nfyYQ9X0+cIa7/z5zvo8TIskfBv5qwo9ERERmPU3lJiIzwRAwauJtd99RYt/3Jh3juM8+4ErC+9lp\nEzjn17Id4+hyoAt4lZm1jj5kNHc/tdQFUL6ziMgsVLeR48SI/NsYWX300UcBuP7664tl++KUZ0cd\ndTQACxak+cFJNLlYl6X5yObhdiFGZgcH0wnSPOYtF4bDcUOZqdkWL14MwOmnn17cNm9eyFF+6KGH\nYpuykeNdI667urqKZfnFPCzTvqTOJLKdPE6Avsw0ciI1dCXwb8A9ZnYVcCNwc5m0hltLbHs0Xi+e\nwHlvzG9w9y4zuw04hzDTxW0TqE9EROqAIsciUlPu/jHgIuBh4G3Ad4FtZvZzMxsVCXb33SWqSb55\nNpYoG8u2MbYnaRmdE6hLRETqhDrHIlJz7v5Vd38asBQ4D/gicDbwYzM7cIpOu2yM7clqPV1jlIuI\nSB2r27SKJLXAMyvCJYPTknSC7DRqyQC+O++6C4AjVqar5x0RV9Lr7AyBpJb2dNBdW3s7APPiYL3s\n+YaSdAcfPVAukaRXAJx2WgiSHXtsGLjX1dVdLNuyJQwefOSRRwDYvj0dt/T444+H+mPaRktmkF/S\nhq3bQpAsm9qRnQ5OZCaIUeEfAj+0MC/iawmd5O9MwenOAb6a3WBmncDJQB+wfrInOPGQTtZqkQoR\nkVlFkWMRqSkzO9eyifKpg+L1VK1w95dm9pTctssJ6RTfcPf+0YeIiEi9q9vIcRK/LfWJm2zNRk6T\nyOymTZsB2LYlTUdsbAzfIVYecQQADX3poLaGPeEpXNC3EID589OBfMmCHcnUbNnzNdnoSHMy8C+Z\n+m3p0iXFskMOORiAVSesAkZONdc/EKZkSwbp3XP3PcWyW35zCwA7doRB/9nodXZBEJEa+i6w18xu\nATYQ/kGfCZwOrAV+OkXnvRa42cy+CWwhzHP8jNiGy6bonCIiMsOpdyQitXYZ8DvgFODNhIU4moH3\nAue6+6gp3qrk4/F8J5OukrcGODM/37KIiMwddRs5ttjvb8xERxtiBLixMTzsgYE0/zaZDm3p4rDm\nwHHHnlAse8ITDgNgydIQHS5kvlIMxWnaCoUQke0fSD/HmxpDWXNTczxvZgq42JZkKjiA5pawaEhj\n0+hc4MGYKzxvYVg8pHvP3mLZYzvClK/74qIeHZ2LimVnPuMcAG5buw6ABx5I86yTNovUkrtfAVxR\nwX6ry5StIXRs89tL/3g0znEiIjJ3KXIsIiIiIhKpcywiIiIiEtVtWkVxsFlmEHwy9m0gDmAbObVa\n3K85DIY7aOUxxZLO5YeHOtvC09Vo6SC61jiwrrEhpEQ0N7Wk54v7OXGFvEKaLtHc0DxiH4D+wdCe\ngX1hkPy+3jR1oq8vbBscCvt4uvAf27aFtIrt28Pqeeeufnax7IAlYaq4/p4wgG/Lls3FsoEBrZAn\nIiIikqXIsYjMKe5+ububu99Q67aIiMjMU7eR40R2qrRCoTBqW6IhTrPW2N4BwINbH0sLO8JAvHlt\noay1KV1ko70lRJqbG8KAuebmdECeNcbzNIxuSzIAsKcnncK1v78/1hGiyi2taaT5wIPCQMED41Rx\ng/3peQZ6w+O6Y93dAHz1S/+VecwhSn7XnbcB6WInsRQRERERSSlyLCIiIiISqXMsIiIiIhLVbVpF\nqdSJUtsSyZy/By1bCsBJJ60qljU1htQJHw6D9gYG0noG+noBaGsNx1t/uuKsxXmNm1tDmkSSLgHg\njeF259IDi9sWLw4r4s2fH9I39u7rLpb17Aur8t13/wYA1t99V7HsD3FFvA0PPgDA5k2b0uN69oTH\n0NwQrzOr9DWVnQJWREREZM5R5FhEREREJKrbyHEy+M5GTOU2duTY4/6P3L8+XB+6olh25FFhtbwl\ni5cB0Nq2oFjWECPAHQtCdHmokA5yGyqE87W0toxqy2BcnW94ON1/8/atAGy7I1zv3LWjWLane3co\n2xymYtv4yMPFsu2bNgKwt2t3fCxp9Lq1JUSKk7OMHKCIiIiIiGQociwiIiIiEtVt5Djp92cjpfnI\ncWNjGslNpnJrijHWgRipBeh6LOTwdu0Mi200d8wvlrUvCFHkgS2FpNK0/uYw5VtTjC43N6TfRZoa\nwn6tjemf4PGt2wC4Y+2tAGzd+kixrKMjRKa7dscI8saHimW7YrsGY75zJkCdLoYSZRcPGdZ3IxER\nEZER1DsSEREREYnUORYRERERieo2raLBQtqCWWYAWm48XjatorEpfE845uijAHjaGacVyzqXhCnW\naArpEe0LFxfLmuKKeg0tIYWiOa6YB+CE+gf7QrrDvu49xbJ9XeH2rsfSlfi2PRKmYhveF1Indm/f\nUix7uHsXAAMDYWW87Ep3heHBeL4kZyJ9XAUfOepuxFMwrBF5IgBmdgNwjrtrfkMRkTmubjvHIiK1\ndtemLlZedk2tmzFhGz58Xq2bICJSM3XcOQ4x0sKIqcvC7WSMWjaSfPCyME3baac/BYDlhywtljU1\nh6nYGuIAu7Z5LelZ4vg7a4pTx3k6jdpQf5yubV8PADs2PVosu/O22wHY+Eg66G7nY48DsKc7LP6x\ne+fjxbKuvSHSPDw8POIa0ini8oPvQgOTq4ktiiIiIiIyFynnWERmFTN7qpldbWabzKzfzLaY2XVm\ndkFmn4vN7Dtm9qCZ9ZpZt5ndbGavztW10kLu1TnxvmcuN0zvIxMRkZmgjiPHSepgGh1NIqtJgHXR\nos5i2SmnnALAYYcfBqQ5yABNcSGNxqYYJi4MFcuGBkO+777eLgAe37GzWLZlY1iwo7srRIK3xana\nALZsDtPDdWfykJOIcW9vWJK6Jy5NDTA0lJ4TSi9ukt1WLIuP3+LzUW5qO5GZzsxeD3wOGAb+B7gP\nOAg4DXgz8M246+eAu4FfAFuApcALgK+Z2XHu/g9xv93AB4CLgcPj7cSGKXwoIiIyQ9Vx51hE6omZ\nnQB8FugGnunud+fKD83cPdHdH8iVtwDXApeZ2RXuvsnddwOXm9lq4HB3v3w/2rV2jKLjJ1qXiIjU\nntIqRGS2eBPhC/0/5TvGAO6+MXP7gRLlA8C/xzqeNYXtFBGRWazuI8fZTIMki6C5OUzJtiwOwgNY\nsWIFAG1tcWq2hubMgSGdwmNmw2BmMFx3dxhsd/vtdwFwx513Fst27gypFsNDYf9du3cVywZiOkZj\nZkW9vfv6AOiJA/gam9I2tLaGxg/G45JrgEIhDga00akTecm++dsis8DT4vW14+1oZocB7yV0gg8D\n2nO7HFKtRrn7qWO0YS1wSrXOIyIi06PuO8ciUjcWxetN5XYysyOB3wKLgZuA64AuQp7ySuAioHXK\nWikiIrNa3XaOG4oD6tLMEY8LYnicw23Xzh3Fss2bHgZg5cqDAWjvWJIeFwOsvQMhsrt9RxoB/uMf\nwq+3d9wZfuXdsnV7sWxfjAD39Q8AsHdPOvguidlmF+lojpHiwRhpbsouUhIjzMkUbtnBd0lZqchx\nfrCeBuTJLLY7Xh8C3Ftmv3cSBuBd4u5rsgVm9kpC51hERKSkuu0ci0jduYUwK8XzKd85Pjpef6dE\n2TljHDMMYGaN7j48xj4TduIhnazVghoiIrOKBuSJyGzxOWAI+Ic4c8UImdkqNsTr1bnyPwVeN0bd\nyYo7h026lSIiMqvVbeS4EBMXLNv/95BakMwZ3N/bk+4/FFImCsP9cZ+BYtngYCi7+54QrLp13e3F\nsk2bw9zFO3fGX3wtHWDXtasr1hUCUaVSGpqa0j9Ba3NMj4gDALOD7pK0iGT/ZFAhpAPrkuvseZLH\nmp8nGcZYUU9khnL3e8zszcAVwO/N7PuEeY6XAqcTpng7lzDd2yXAt8zs28Bm4ETgeYR5kC8sUf31\nwCuA/zazHwK9wMPu/rWpfVQiIjLT1G3nWETqj7v/p5ndBbybEBl+CbADuAP4QtznDjM7F/i/wHmE\n97nbgZcR8pZLdY6/QFgE5M+Bv43H3AhMpnO8cv369Zx6asnJLEREZBzr16+HMJB6WpkGZYmIVJ+Z\n9QONhI65yEyULFRTLodfpJaeDAy7+7TOMKTIsYjI1LgLxp4HWaTWktUd9RqVmarMCqRTSkmnIiIi\nIiKROsciIiIiIpE6xyIiIiIikTrHIiIiIiKROsciIiIiIpGmchMRERERiRQ5FhERERGJ1DkWERER\nEYnUORYRERERidQ5FhERERGJ1DkWEREREYnUORYRERERidQ5FhERERGJ1DkWEREREYnUORYRqYCZ\nHWpmXzKzzWbWb2YbzOwTZrZ4gvUsicdtiPVsjvUeOlVtl7mhGq9RM7vBzLzMpW0qH4PULzN7uZl9\n2sxuMrPu+Hr6r/2sqyrvx2NpqkYlIiL1zMyOAn4FHAR8H7gXeCrwduB5ZnaWuz9eQT1LYz3HAj8D\nrgKOBy4BzjOzp7v7g1PzKKSeVes1mvGBMbYPTaqhMpf9PfBkYC+wkfDeN2FT8FofRZ1jEZHxfZbw\nRvw2d/90stHMPgZcCvwz8MYK6vkQoWP8MXd/V6aetwGfjOd5XhXbLXNHtV6jALj75dVuoMx5lxI6\nxfcD5wA/3896qvpaL8XcfTLHi4jUtRiluB/YABzl7oVM2QJgC2DAQe6+r0w984HtQAFY4e57MmUN\nwIPA4fEcih5Lxar1Go373wCc4+42ZQ2WOc/MVhM6x1e6+6sncFzVXuvlKOdYRKS8c+P1ddk3YoDY\nwb0Z6ACeNk49TwPagZuzHeNYTwH4ce58IpWq1mu0yMwuNLPLzOydZvZ8M2utXnNF9lvVX+ulqHMs\nIlLecfH6j2OU3xevj52mekTypuK1dRXwL8C/AT8EHjGzl+9f80SqZlreR9U5FhEprzNed41Rnmxf\nNE31iORV87X1feBFwKGEXzqOJ3SSFwFXm5ly4qWWpuV9VAPyREREBAB3/3hu0x+AvzOzzcCnCR3l\nH017w0SmkSLHIiLlJZGIzjHKk+27p6kekbzpeG19gTCN28lx4JNILUzL+6g6xyIi5f0hXo+Vw3ZM\nvB4rB67a9YjkTflry937gGQg6bz9rUdkkqblfVSdYxGR8pK5OJ8bp1wrihG0s4Ae4JZx6rkF6AXO\nykfeYr3PzZ1PpFLVeo2OycyOAxYTOsg79rcekUma8tc6qHMsIlKWuz8AXAesBP4mV/wBQhTta9k5\nNc3seDMbsfqTu+8Fvhb3vzxXz1ti/T/WHMcyUdV6jZrZEWa2JF+/mR0IfDnevcrdtUqeTCkza46v\n0aOy2/fntb5f59ciICIi5ZVYrnQ9cAZhzs0/Amdmlys1MwfIL6RQYvno3wKrgPMJC4ScGd/8RSak\nGq9RM7sYuAL4JWFRmp3AYcALCLmctwLPcXflxcuEmdlLgJfEu8uBPyW8zm6K23a4+7vjviuBh4CH\n3X1lrp4Jvdb3q63qHIuIjM/MngB8kLC881LCSkzfBT7g7rty+5bsHMeyJcA/Ej4kVgCPA9cC73f3\njVP5GKS+TfY1amYnAe8CTgUOBhYS0ijuBr4J/Ie7D0z9I5F6ZGaXE977xlLsCJfrHMfyil/r+9VW\ndY5FRERERALlHIuIiIiIROoci4iIiIhE6hyPwcw2mJmb2eoJHnd5PG7N1LQMzGx1PMeGqTqHiIiI\nyFykzrGIiIiISKTOcfXtIKzgsqXWDRERERGRiWmqdQPqjbt/BvhMrdshIiIiIhOnyLGIiIiISKTO\ncQXM7DAz+4KZPWpmfWb2kJl91Mw6S+w75oC8uN3NbKWZrTKzr8Q6B83se7l9O+M5HornfNTM/tPM\nDp3ChyoiIiIyp6lzPL6jCUtm/hWwCHDCmt7vAm41sxX7UeczY52vISzJOWKd+ljnrfEcK+M5FwGv\nA9YBI9YaFxEREZHqUOd4fB8FuoBnuvsCYB5h2dcdhI7zV/ajzs8CvwNOcveFQAehI5z4Sqx7B3A+\nMC+e+2ygG/i3/XsoIiIiIlKOOsfjawWe7+6/BHD3grt/H7gglj/HzJ4xwTq3xzrvinW6uz8AYGbP\nBJ4T97vA3f/H3Qtxv5sI64i3TeoRiYiIiEhJ6hyP75vufn9+o7v/HPhVvPvyCdb5GXfvHaMsqeuW\neI78ee8Hrp7g+URERESkAuocj++GMmU3xutTJljnr8uUJXXdWGafcmUiIiIisp/UOR7fpgrKDpxg\nnY+VKUvq2lzBeUVERESkitQ5ro3hWjdAREREREZT53h8B1dQVi4SPFFJXZWcV0RERESqSJ3j8Z1T\nQdm6Kp4vqevsCs4rIiIiIlWkzvH4LjSzI/Mbzexs4Kx491tVPF9S19PjOfLnPRK4sIrnExEREZFI\nnePxDQDXmtmZAGbWYGYvAr4dy3/i7jdX62RxPuWfxLvfNrMXmllDPPdZwI+A/mqdT0RERERS6hyP\n793AYuBmM9sD7AX+hzCrxP3ARVNwzoti3QcCPwD2xnP/krCM9LvKHCsiIiIi+0md4/HdD5wGfImw\njHQjsIGwhPNp7r6l2ieMdZ4OfAx4OJ6zC/giYR7kB6p9ThEREREBc/dat0FEREREZEZQ5FhERERE\nJFLnWEREREQkUudYRERERCRS51hEREREJFLnWEREREQkUudYRERERCRS51hEREREJFLnWEREREQk\nUudYRERERCRqqnUDRETqkZk9BCwkLDcvIiITtxLodvcjpvOkdds5fs+HznGAhoY0ON7YZOFGfNRO\nunS2FwoAFAphn+yq2m5DcVusK1NYiLc9OW44cxxDxVvhsNHHFYbTA4YLYdvwcGjL8GAhLRuK22KV\nyT5hWzhuYCDUNTgwVCwbiLeHBsM+Q5njCvHUv752kyEi1bawvb19yapVq5bUuiEiIrPR+vXr6e3t\nnfbz1m3nONt5TFjswCZ91hE9wqTf6oV4N9OJjD3eQmEw1O3Z42IthcZwN9M5LiR1+ejOcXK7UMh0\nmGPFQ7Gzm3R6Q9nIx5V0lsO2pK6kg5/t2Sc3QjsbLJNJo6Qakam0YdWqVUvWrl1b63aIiMxKp556\nKuvWrdsw3edV90hEZgwzW2lmbmZrKtz/4rj/xVVsw+pY5+XVqlNERGYPdY5FRERERKK6TasoLaYW\nxHQHLE2ssOS2hfyFJAcZwAabAWiKqRPNDenTNjA8AMCQ94XjSPMqkipGpDlEXkhyldNthZju4cM2\nuqyQ5Cgneck+qix9LKPvNDSMTis2ZRrL7Pdd4BZgS60bUspdm7pYedk1tW6GiEhNbPjwebVuwn6Z\nY51jEakn7t4FdNW6HSIiUj/qtnOcRIKtRHQ4vU6zSpK93EN0uKVxQbHs0AOeCEDjYDsA27Y/Vizr\nGe4ONzr6Adjdu7VYNjwcosmlIsckkePBzKbhJJocB9ZlZ6SI24oD8jJlpQb8JYoR42SmjewMHa7Q\nscxcZnY88GHgbKAV+D3wQXe/LrPPxcCXgUvcfU1m+4Z480nA5cDLgEOAf3b3y+M+y4APAS8kTLn2\nB+DjwMNT9qBERGTGq9vOsYjMakcAvwbuBP4DWAFcCFxrZq9y96srqKMF+BmwBLgO6AYeAjCzA4Bf\nAUcCv4yXFcAVcd+KmdlY01EcP5F6RERkZqjbznESE23IRI4b8oXZpFtL5ikOOcMLWhcXi554xOkA\nPPrITgC2brmvWLZ0aScAHS2HArBr795iWWG4P16H+17IRm3jdXa6tuJUbEnkOBMJTgLFSR2FbOR4\n5HV2kjor5hw3xPtj5yqLzCBnAx919/ckG8zsM4QO8xVmdq27d49TxwrgHuAcd9+XK/sQoWP8CXe/\ntMQ5RERkjtJsFSIyE3UBH8xucPdbgSuBRcBLK6znXfmOsZk1A38B7CGkXJQ6R8Xc/dRSF+DeidQj\nIiIzgzrHIjITrXP3PSW23xCvn1JBHX3AHSW2Hw90ALfFAX1jnUNEROaguk+rMEpMYVa8lU67lgxm\na4ir6M1rSAfkFfrDHGt7uncB0NKYfqcY7g2pE/ffsx6AoY6dxbKm1rjsdDxNJhOiOE3biOWjh5Np\n5OK0bSPXsB5RlhlXV8wOSR7ryEXwbMRO2UwKa1BahcxY28bYnox47aygju1eapRqeux45xARkTlI\nkWMRmYmWjbF9ebyuZPq2sb79JceOdw4REZmD6jZynAzEs0zgqCGZzqw4hdnoyPGC5oMAOHTpkcWy\nwZ4QHW5vCcd1dMwvlm1//HEA+glpjW2taRuGkkU94g0vpDHrQiG0ZSgzJVsydVsxYjzio33shT4s\nTud5BxAAACAASURBVNeWjjNMv/Oks7UlC6Bka9F3I5mxTjGzBSVSK1bH699Pou57gR7gZDPrLJFa\nsXr0IfvnxEM6WTtLJ8EXEZmr1DsSkZmoE3h/doOZnUYYSNdFWBlvv7j7IGHQ3QJyA/Iy5xARkTmq\nbiPHIjKr/QJ4nZmdAdxMOs9xA/CGCqZxG8/fAc8C3hE7xMk8xxcCPwRePMn6RURklqrbznEyEM2y\nA9CKt0oMavM2AJYtPhyARu8olq1ddxsAiw4M43iGMykND2/fDUD3QBjbc1hTOk6oo7kFgEJhT7zO\nrHgX5zDOzjWcGZs3sp0l2Ii8iuLG0PZMmVtuHx89t7PIDPQQ8EbCCnlvJKyQt46wQt6PJ1u5u+8w\ns7MI8x2/CDiNsELem4ANqHMsIjJn1W3nWERmH3ffACOmmDl/nP3XAGtKbF9Zwbm2Aq8do1jfHEVE\n5qi67RyXmsItzz1NuW5pXAhAW/MSAB59ZEta1hGiykcefQwAj+1OxwhtfTQMyHt8T4gg79k2WCxb\neeQiABYdECLIw4V0LYIkijw8IpocbicB3YbsQyjOyGYjrsOO8fEU982WJY+xRBRaH/8iIiIiI2hA\nnoiIiIhIVL+R4xL5tMVZ3bwxXqdR20baARgeDHOxDQylszstWxamQ926dTsAt9+1vlg20N8LQEdT\nWDRk12M9xbLhQogmH98apodrbR3ItKU3NiET0R0auWCHZ/86DfnHNTpynG7J3Ir7N8Tp3QqmnGMR\nERGRsShyLCIiIiISqXMsIiIiIhLVbVqFF6csS1MnkiyK4gpymQF5HgfGDQ6F+dT27E0Hzw0Obg7b\n+vtjWW+xbOHCkI7R0hzSMXY3NWSOC7c3PxTqWrS4sVi2oHNhbEyahuEe6jca4/20ruJKfzbiiuw9\ni4PvsiklDXFbMr1bdia3Fo3IExERERlBkWMRERERkahuI8fFgHF20FkSfC2OgUu/G/QNhOnZNm17\nGIBHNqVTuXW0hQVBeodCpT1702gvg2HqNovzrrW1pAPs+rvDflseDZHmPbvThUUOP2IFAM0d6f7W\nNJhtJlbIDrrzEduy0eFk2rpk0F1DZg64xsYYrW6M2zKDENtd341EREREstQ7EhERERGJ6jZybDG5\ndsQ6GsWp3GL+bSYBt0CYZq25LURWW9ubi2Xde/cCMGghCtvckpbt3rUzHh+jyr1pVLngsc7WGNFt\nSnOOH9uxJ5ala0Z3Lg6LjbR1hG1eGErbHiPGPmpKt3Sp7CQ4nESQAZpiWUt8rEPZskLaHhERERFR\n5FhEREREpEidYxERERGRqG7TKigx6M5i6kMygM0zZU4YDDcUp1Y76riVxbI/rH8QgK6dIRWib6i/\nWLZwyeJwfFx1bzC7rF1DSKtIsjdaOtrToqawnw+3Frf1xdnjmprDAL7GlsyKeoU4TVtyfEPa9gZL\nUkhGT+WWjA88bt5SAHb29hXLBnoHEREREZGUIsciMqOY2QYz21DrdoiIyNxUt5Hj4nRo2anc4oA6\nSgzWa4xl/b27w4amdIq11nkh4jy8PURyGzPj2NpaWgDo7QlR2M5FC4tlhRhN7o5Tv/UPpYPvBgsh\n+pxMEwcw2BAiy/194TvL/Ox4Oesf8XgaMq1viCMNk4F4jZkp2pYuDO1pa1wQztefRosXN7chIiIi\nIilFjkVEpshdm7pYedk1rLzsmlo3RUREKqTOsYiIiIhIVLdpFemAvIxkxbnifMCZsphWMRznJt7X\nt6NY1NAUvkO0toZKFy1M0xGG+sIAtwXtobK+oX3p6Qph//kdYV5kt3R+5IGBkGLR3Jz+CRYvOQCA\npQtCekVz495i2d7+beF8ha7YqHRQIA0t8To+FEvTN5rbwoC/fYQ5k3uG0gF5+3rTAX8i08lCftDf\nAG8CjgIeB74LvK/MMa8E/hp4CtAGPARcCfyru/eX2P944DLgWcAyYBdwPfABd/9Dbt81wEWxLecB\nrweOAX7j7qv3/5GKiMhsU7+dYxGZyT4BvA3YAnweGATOB84AWoAR39zM7EvAJcBG4DvAbuBpwD8B\nzzKz57j7UGb/5wH/DTQDPwDuBw4FXgacZ2bnuvu6Eu36JPBM4Brgh8BwiX1ERKSO1W/nOJk/rZDZ\nliSRJFOf2egV4hqT6Gsm9NwwL2xccfB8AHY/3lss64v7DQ/Hz1BPB7y1x6hwMg4vu0Jec2ccuNfY\nUtzW0hyCX+7hgOFC2viOtjAV21ChKT6stA0Wp3VraByK99Pjdu75/+zde5zdVX3v/9dn77nfM5M7\nwSRcE0EQUrlJBUSUHlvx0eKxlrYH/fVUW+qV9lEEzwG8YdVaFe3psZbq0Sr0aD22KsVWQQVEMCgY\nCNcQCAlJyGUySea6Z39+f6z1vWRnz2SSTGYme97PxyOP757vWt+1v3vYzHz2Zz5rrbD8XF1zyBg3\nt2T38PSzWXZcZKqY2XmEwPhp4Cx33xHPXwfcCSwCns31v5IQGH8LuMLdB3JtNwDXE7LQn4nn5gBf\nB/qBV7n7o7n+pwL3AV8Ezqxye2cCZ7j7MwfxelaP0bRiomOIiMjMoZpjEZlqb43HjySBMYC7DwLv\nr9L/3UAJeFs+MI4+RCjJuCJ37g+BLuD6fGAcn2MN8PfAGWb20irP9fGDCYxFRKT21Gzm2KpljpOS\n4/L+S7klX5klqePchcXwuL09fLvqrT1t6i+GcwN7Q9a3nKt1NuLycOWQ0R0dzep9O9rCGOVCf3qu\n5CHLuycuC1cqZfdQLIY657q6cCzk7qEu1hPXWby/YvaZZzR+A0ZjNrqlPrcEXEl/MZZpkWRsf1Sl\n7W5ypQxm1gKcDmwD3rPP0oyZIWBl7utz4/H0mFmudFI8rgQerWi7f7wbr8bdV1U7HzPK1bLTIiIy\ng9VscCwiM1ZnPG6pbHD3kpnl633mED65ziOUT0xETzz+9wP0a6tybvMEn0NERGqUyipEZKrFJVdY\nUNlgZnXA3Cp9f+HuNt6/KtecfoBrvlzl3qqtcyMiIrNI7WaOk4oEq/K7Ll3yLDuVPPZkd7n8ZV6O\nfUL5QlNrdmFTnHQ3PBhLGvqyCW+9O8OyboVymHjfPa85u64plE6MjOSWXYtLsJU8PHkpNyEvlGOC\nxRIKzxWFDA2E/kO7wlJxddkcP1rnhn59MXZoasgtHTe/E5Fp8CCh3OACYF1F2/lA+j+Ru+8xs0eA\nU8ysO1+jPI77gN8hrDrx8OTc8qE59ZhOVn/s9dN5CyIicpCUORaRqfaleLzOzLqTk2bWBNxUpf+n\nCMu73WJmXZWNZjbHzPK1vf9IWOrtejM7q0r/gpldeOi3LyIitax2M8cx+7rPX1stXcst9tlnFxAA\nyjHl7LkJeUmvJLtc8CzbWxcn942Ohv6duSGH4wS89sUhY9zSkjXaUJjA19SYbQyyJ2aRy/G+6nIf\nXbwQ2urif7GGhuw6I25KUgrnBgazSX4+GsYajjue7Mgt29owp3b/88vM5e73mNnNwDuBNWb2DbJ1\njncS1j7O97/FzFYBfwo8bWZ3AM8B3cBy4FWEgPgdsf92M7ucsPTbfWb2A+ARwt+DjiVM2OsBmhAR\nEamg6EhEpsO7gScI6xO/nWyHvGuBhyo7u/tVZnY7IQB+DWGpth2EIPkTwFcr+v/AzE4D/hx4HaHE\nYhjYBPyQsJGIiIjIfmo4ON5/yac0i+zV+oTHyf4ZhVybp5nckMptKmUp3fUPPw1AKfbvaGpJ2xa3\nhI0+uuaGvxwXc1s319eFOuSGYlbc3NcYJtlvLYW64q07t+ZuL9xYXUwdFwpZbXOSTa5rCWO1drVm\nl6XL14W2ltx1hYFse2qRqeTuDnwu/qu0bIxrvgN85yCeYz3wZxPseyVw5UTHFhGR2qWaYxERERGR\nSMGxiIiIiEhUs2UVfoirlVr8vJC/vBy/aoj1C12N2WcK6w+T39o754S29o60bevuPgA6d4Ryiq7W\nbI21vYPhXP/QcHpuwYqlAOzqC+UUxfrc88QbKsbd7+pys/WKsVKiEEs0ivmPPHEiXjlOzOsqNqZN\nrR37TfwXERERmdWUORYRERERiWo4cxwypWb7T7obTzlmWku5zUM8Zo59NGR59/SX0ralJx0PQFN3\n2Oxr89YX07Ytu3tD/3JYtm1oYZapLXaESXMjjekyr7zw7EYABut3A1BXl98EJHyOKRRtnyNAsS6k\njpOscqGQm0wYX/9oPO7x7N7ds6y1iIiIiChzLCIiIiKSUnAsIiIiIhLVbFlFMrHOcqUUln4WqDJb\nL5YdFOJMvjqykoZifNw+GtsK2betPpZHPPPCFgB29valbeU4xI64vnF7tqkdTY0j4flyJRDDu0O/\ncl0sCfHcmsSxTKSYlFUUss81hbjzX8GKFa8TCvGlJjv57c2VVdQVD3HWooiIiEiNUuZYRERERCSq\n2cxxIWaCLZeZTR4nWVjPfTRIJu7VxWSq5zLOjbGt2B+yvZ5tdMfTGzcAsGHLLgB27s4yx8XGcF1r\na2ccKHvCIcIScJSyXeoKzXH3u3hjdZ6lmpMd7rIscXZ/SRY5acvPO0yXgItp7JFc1nygMZfKFhER\nERFljkVEREREErWbOS4mmePsXPK4HB805OqKC/FxOe6oMZr71jQwCoDHeuQXNu1K2559YScAdfUt\nADQ2jaZt3QvCxiCD5QEAtmzZnrYtPm5eeFDM+pdjdrcYs8RWzi3JVti31riQe2FJrySZnF+9LtkM\nxWLx8Wj2khmq02cjERERkTxFRyIiIiIikYJjEREREZGoZssqksl3xfymePGjwKiFUoaGwaykoa4x\nfCsG6kPdQf1obrm2oTARryEu5bZ31960bbQUxuhoDeUY9TSkbS0N4cnnz18IwObdW7JbiWURlvsv\nYKPJrn7JmdwuffFhIS2vyE003GcXQKg6Iy9Zqi7X5lrJTY5CZrYewN2XTe+diIhILVLmWEREREQk\nqtnMcbL7RX7Js4aYKq2Ps9IGtvSmbfU9YfJcfcNwaNu8OW3zQiMA5bj5R8/ctrRtuD2M2RS/k3Wj\nzWlbvw0B0DEnNDbM7U7binXhHkYrk76QZnk9l9otlyvTvL7fY6+SCk7OJS35JHO52mYoIiIiIrOY\nMsciIiIiIlHNZo7T/TByqdL6wZAVbu0LS6sN7R5J23qHdwPQ0hYuLG3ONvPYQVimrRCzy3ty37X6\n9pApTkp7CwO5rZvjZiG9O8Nyb+2LW9O2JKObz+RmS7H5Pq8B8hnwJBOcrcnmPva22Fk2OdmSOldz\nXC1rLTIDWPgf9yrgT4Djge3At4DrxujfCLwXuCL2LwEPATe7+z+PMf67gLcDx1WM/xCopllEZLaq\n2eBYRI5qnyYEry8AXwBGgMuAs4EGYDjpaGYNwB3ABcBjwOeBFuBy4DYze7m7X1sx/ucJgfemOP4w\n8AbgLKA+Pp+IiMxCCo5FZEYxs/MIgfHTwFnuviOevw64E1gEPJu75GpCYHw78AZ3L8X+NwL3A+83\ns++4+73x/K8TAuMngLPdvTeevxb4T2BxxfgHut/VYzStmOgYIiIyc9RucBxLE9yyUoNCXVhubaAv\nLMW25YVsQt5eQunDwoWhTKKU20pu4/awBFupL0ywKzdk9Qjb2QFAa08ovWjPLeXW0Rgee2kk3lJW\nJ+GWjL//xLqkgqJQpezB0zLx/JJsyYS80Xh91Vl+7DP4vkOIzCRvjcePJIExgLsPmtn7CQFy3tsI\n//O8LwmMY/+tZvYh4IvAHwH3xqb/lhu/N9d/OI5/96S+GhEROarUbnAsIkerM+PxR1Xa7gbSBcrN\nrB04Adjo7o9V6f/DeDwjdy55XC0Ivo9Qrzxh7r6q2vmYUT6zWpuIiMxcNRscW5KFzWVmrSFkcks9\nPQD0btqTtjW1hcyvzQ2T5ort2QYhVgq/K1/cGSbp7dw6lLU1hgzwnsFYoriwM21bsmweAC2tYSm4\nUiH7nZvkjY39N/Nwr9wMJP+6Cvv0ARgdTa6LffIbhKTjJ2Pm20RmpOR/oi2VDe5eMrNtVfq+MMZY\nyfmuCY4/ambbD+JeRUSkxmgpNxGZaXbF44LKBjOrA+ZW6btwjLEWVfQDSJaiqTZ+EeiZ8J2KiEjN\nUXAsIjPNg/F4QZW284Fi8oW77yZM3DvGzE6s0v+iijEBfpEbq9I51PBf1ERE5MBq9pdAXSH8/iyW\ncrvMeSh9KHeG8opFp78kbWtpDWUVHZ1hLeNd27K/rPrmsAby5v4wN8hbmtK25qH4+WJnWFmq48Ss\nrKKhM4w5SjJRLvssUkxua59d7fadNJdfh7iye7mcTRgcGU7GLcbL9y/VKMRSi7Jn15Vzkw5FZpAv\nESbQXWdm386tVtEE3FSl/y3AR4BPmNnveJyZamZzgf+R65P4P4RJfMn4u2L/BuCjR+D1iIjIUaRm\ng2MROTq5+z1mdjPwTmCNmX2DbJ3jnexfX/xJ4Ddi+0Nm9j3COsdvAuYDH3f3u3Pj/8jMvgD8MfCI\nmX0zjv9bhPKLTcBkfHJctnbtWlatqjpfT0REDmDt2rUAy6b6ec19/13VRESmU26HvKvYdwe7a6my\ng13MKr8P+D323SHv8+7+9SrjF4B3E3bIW14x/vPA0+7+8sN8DUOEP+c8dDjjiEyCZM3taiu6iEyV\nQ3kfLgP63H355N/O2BQci4hEsW75CeBWd3/LYY61GsZe6k1kqui9KDPB0fQ+1IQ8EZl1zGyh5ScB\nhHMthG2rIWSRRURkFlLNsYjMRu8B3mJmdxFqmBcCFwNLCNtQ/9/puzUREZlOCo5FZDb6D+B04LVA\nN6FG+Qngs8CnXfVmIiKzloJjEZl13P0HwA+m+z5ERGTmUc2xiIiIiEik1SpERERERCJljkVERERE\nIgXHIiIiIiKRgmMRERERkUjBsYiIiIhIpOBYRERERCRScCwiIiIiEik4FhERERGJFByLiIiIiEQK\njkVEJsDMlpjZLWa2ycyGzGy9mX3azOYc5Djd8br1cZxNcdwlR+repbZMxnvRzO4yMx/nX9ORfA1y\n9DOzy83sZjP7iZn1xffNVw9xrEn5+TpZ6qbjSUVEjiZmdjxwLzAf+DbwGHAW8G7gUjN7pbtvn8A4\nPXGck4AfArcCK4C3Aq83s3Pdfd2ReRVSCybrvZhz4xjnS4d1ozIbfAA4HdgDPE/4WXbQjsB7+rAp\nOBYRObC/Jfzgfpe735ycNLNPAe8FPgK8YwLjfJQQGH/K3a/OjfMu4DPxeS6dxPuW2jNZ70UA3P2G\nyb5BmTXeSwiKnwIuAO48xHEm9T09Gczdp/L5RESOKjGr8RSwHjje3cu5tnbgBcCA+e6+d5xx2oCt\nQBlY5O67c20FYB2wND6Hsseyn8l6L8b+dwEXuLsdsRuWWcPMLiQEx//k7r9/ENdN2nt6MqnmWERk\nfBfF4/fzP7gBYoB7D9ACnHOAcc4BmoF78oFxHKcM3FHxfCKVJuu9mDKzN5vZNWb2PjP7DTNrnLzb\nFTmgSX9PTwYFxyIi4zs5Hp8Yo/3JeDxpisaR2etIvIduBW4C/hr4HvCcmV1+aLcnctBm5M9FBcci\nIuPrjMddY7Qn57umaByZvSbzPfRt4LeAJYS/aKwgBMldwG1mptp3mQoz8ueiJuSJiIjMMu7+NxWn\nHgeuNbNNwM2EQPnfp/zGRGYAZY5FRMaXZC46x2hPzvdO0Tgye03Fe+iLhGXcXh4nRIkcSTPy56KC\nYxGR8T0ej2PVvJ0Yj2PVzE32ODJ7HfH3kLsPAsmE0dZDHUdkgmbkz0UFxyIi40vW7nxtXHItFTNr\nrwT6gfsOMM59wADwysqMXBz3tRXPJ1Jpst6LYzKzk4E5hAB526GOIzJBR/w9fSgUHIuIjMPdnwa+\nDywDrqpovpGQXftKfg1OM1thZvvsFuXue4CvxP43VIzzZ3H8O7TGsYxlst6LZrbczLorxzezecA/\nxi9vdXftkieTwszq43vx+Pz5Q3lPTwVtAiIicgBVtjddC5xNWKPzCeC8/PamZuYAlRssVNk++n5g\nJXAZYYOQ8+IvC5GqJuO9aGZXAn8H3E3YfGYH8BLgvxBqPH8OXOLuqn+XMZnZG4E3xi8XAq8jvJ9+\nEs9tc/c/j32XAc8Az7r7sopxDuo9PRUUHIuITICZHQt8kLC9cw9h56ZvATe6+86KvlWD49jWDVxP\n+KWyCNgO3A78T3d//ki+BqkNh/teNLOXAVcDq4DFQAehjOIR4J+B/+3uw0f+lcjRzMxuIPwsG0sa\nCI8XHMf2Cb+np4KCYxERERGRSDXHIiIiIiKRgmMRERERkUjBsYiIiIhINOuCYzNbb2ZuZhdO972I\niIiIyMwy64JjEREREZGxKDgWEREREYkUHIuIiIiIRAqORURERESiWR0cm1m3mX3KzJ4xsyEz22hm\nf29mi8a55iIz+xcz22xmw/H4LTN79TjXePy3zMxWmtmXzWyDmY2Y2f/L9ZtvZp8wszVmttfMBmO/\ne83sg2a2dIzx55nZTWb2KzPbE69dY2YfibtxiYiIiMgEzLod8sxsPbAU+APgw/FxP1AEGmO39cCZ\nVbbh/DBwXfzSgV2EfeiTbTk/5u7vr/KcyTf5Dwn72bcQtuqsB+5w9zfGwPenhO1kAUaBPqArN/6f\nuPvfVYx9PmEv8iQIHgbKQFP8egNwibs/Ps63RURERESY3Znjm4GdwHnu3gq0AZcBvcAyYJ8g18x+\nlyww/hww393nAPPiWADXmNnvj/Ocfws8ALzM3TsIQfLVse16QmD8FPAqoMHdu4Fm4GWEQH5zxT0t\nBf6NEBj/L+DE2L81XvN94FjgX8ysOJFvioiIiMhsNpszx1uAU9x9e0X71cAngWfc/bh4zoAngBOA\nW939LVXG/RrwFkLW+Xh3L+fakm/yOuBUdx+ocv2jwErgd939tgm+lq8CVzB2xrqBEIyfBrzJ3b8x\nkXFFREREZqvZnDn+QmVgHCU1wMvNrDU+fjkhMIaQwa3mxnhcBpw1Rp/PVQuMo754HLPeOc/MWoA3\nEUooPlWtj7sPA0lAfMlExhURERGZzeqm+wam0QNjnN+Ye9wF7AXOjF+/6O6PVLvI3R83s43AMbH/\nfVW6/XSc+/kecDbwV2Z2IiGovW+cYHoV0ECoff5VSG5X1RyPx47z3CIiIiLC7M4c76520t0Hc1/W\nx+O8eNzI+J6v6F/pxXGu/SvgXwkB758CPwT64koVf2FmXRX9kwyzAQvG+dcR+7Uc4N5FREREZr3Z\nHBwfiqYDdxnX6FgN7j7k7pcB5wIfJ2SePff1E2Z2eu6S5L/dLne3Cfy78DDvXURERKTmKTiemCTj\ne6DShCUV/Q+au9/n7n/p7ucCcwiT/J4jZKO/mOu6JR47zKzzUJ9PRERERDIKjifmwXhsNbOqk+3M\n7CRCvXG+/2Fx973ufivwx/HUqtwkwZ8DJUJZxaWT8XwiIiIis52C44n5JWH9YYBrx+hzQzyuB+4/\n2CeIy66NJZmUZ4SaZNx9N/DNeP6DZtY+zth1ZtZ2sPckIiIiMtsoOJ4AD4tBfyB+eZmZ3WxmPQBm\n1mNmnyWUPwB8IL/G8UFYY2YfNbNXJIGyBWeRbTLyQMWufdcAO4CTgHvN7FIzq89de6KZvQ94DPi1\nQ7gnERERkVllNm8CcpG73zVGn+Sbstzd1+fO57ePLpNtH518yDjQ9tH7jFfRpzeOBWHi3i6gnWzF\njG3Axe7+cMV1ryCszbw4nhohrJncTswyRxe6+4+qPbeIiIiIBMocHwR3/wBwMfBtQrDaBmwnLMH2\nmmqB8UG4DLgJuAfYFMceBh4GPkbYze/hyovc/QFgBfCXwL3AHsL6zP2EuuTPAhcoMBYRERE5sFmX\nORYRERERGYsyxyIiIiIikYJjEREREZFIwbGIiIiISKTgWEREREQkUnAsIiIiIhIpOBYRERERiRQc\ni4iIiIhECo5FRERERCIFxyIiIiIiUd1034CISC0ys2eADmD9NN+KiMjRahnQ5+7Lp/JJazY4/sXa\nNQ7w5Lp16bl7/+OHAFz+hjcC0NTZnrb1D+4FYMuG5wB4au3jadtLV6wAYP6ihQA0tDSnbR1zugHo\n6uoJX3d0pG2NjY2T8lqmgE33DYjUoI7m5ubulStXdk/3jYiIHI3Wrl3LwMDAlD9vzQbHIjK1zGwZ\n8AzwZXe/clpvZmZYv3Llyu7Vq1dP932IiByVVq1axYMPPrh+qp+3ZoPjQrEBgHJpND23ZcNGAB79\nefhlNVrwtK1UGgbA4qkdL25L25pWhQxzV898AAaGR9K2gYGReH3o39/fn7a1t4frisUiAA0NDWlb\n/vGRZGb7HN3zr7k0pfciIiIiMtPVbHAsIjLd1mzcxbJrvjvdtyEiMmXWf+z1030Lh02rVYiIiIiI\nRDWbOS7WhVKB0049LT331ImhnOJHt98BQO+e3uyC0VB+0doWSiFOP/fctGnp8uMB6F6wCICRkaxU\nAw+PnVCiUChkc9vGK2kYjc+3z0y4/abFHf48uex+CvvdQ/6xyGSK9ccfA14DtAFrgBvc/TsV/RqB\n9wJXAMcDJeAh4GZ3/+cqYz4DfBn4KPAh4CJgLvBqd7/LzI4DrgFeDRwDDAAbgXuA69x9e8WYbwH+\nGDgDaIrj/xPwCXcfOuxvhIiIHHVqNjgWkWmzFLgfWAd8BegG3gx828xe4+53AphZA3AHcAHwGPB5\noAW4HLjNzF7u7tdWGf944GfAE4RAthnoM7NFwAOE5dO+B3yTEPAuB/4A+ByQBsdmdgvwVuD52LcX\nOIcQdF9sZpe4e2mSviciInKUqNnguFgIk+AWdM9Lzy1ftgSAx++/G4DjlixM29qaWgB4YcsWAPp3\n7UjbSkMhgdTZ0QnAwFCWOS57mJBXXywDsHv37rRtaChM8uvpCSs5lUfLaZuXQ9a2bLbfueSUMbHM\nbpIANts/05xkh5Ox8iOWlTmWI+NCQpb4xuSEmX0N+HfgL4A74+mrCYHx7cAbkkDUzG4kBNfvU4n4\nJAAAIABJREFUN7PvuPu9FeOfD9xUGTib2TsJgfh73P0zFW2tQDn39ZWEwPhbwBXuPpBruwG4HrgK\n2GecasxsrOUoVhzoWhERmXlUcywik+1Z4MP5E+5+B/AccFbu9NsIn9fel8/QuvtWQvYW4I+qjL8F\nuLHK+cR+i2K6+958AAy8m1DC8baK88Tn3k4o9RARkVmmdjPH8VgezmV568LvwONPC0uyveyEU9K2\njtZQa/zkE2Hzj3Xrnk/b/u0rtwDw0rNfBcCqC16dtrV0toWxk/riuvrsJmJitq4Yzo3E5eIA3MLn\nktFcXXE51i+XhkM/H83+olusSAp7OZdx9uTpkszzOLXKhfznodC/ualp7P4iB++X7j5a5fwG4FwA\nM2sHTgA2uvtjVfr+MB7PqNL20Bj1wP9KqEX+vJm9jlCycQ/wqOcK7M2sBTgd2Aa8Z4z/X4aAldUa\nKrn7qmrnY0b5zImMISIiM0fNBsciMm16xzhfIvtrVWc8vjBG3+R8V5W2zdUucPdnzews4AbgUuC3\nY9MGM/uku382fj2HMNt1HqF8QkREJKWyChGZDrviceEY7Ysq+uWNWSzv7mvd/c1AD/BrhJUrCsBn\nzOz/qxjzF+5u4/07qFckIiI1oWYzx8lfSouNWZnD4mXLAWipC3+RPW7horStPpZDjBL+GvzUk8+l\nbT/90U8AuOdnvwBgaDTbIW/xCWGZN4+74B27ZEnatm3ri+G4PUzuGy3lyiSK4XPJyEg21tBQLH0s\nh3lDRctiAPN4Lj4PVkzbisW6fdqSsQHq6ur2OVoxuy6ZkDenqwORqeTuu83saeA4MzvR3Z+s6HJR\nPD54iOOXgNXAajO7F/gx8EbgH9x9j5k9ApxiZt3uvmO8sQ7Hqcd0sroGFsQXEZlNlDkWkelyC6G8\n4RNm2ac9M5sL/I9cnwkxs1Vm1lmlaUE89ufOfQpoAG4xs/1KN8xsjpmpXlhEZBaq2cxxOcm61md/\nGW3ripnigZChbetoTNsamsLjnuE4Ua6UfW7YuTUsjbrwJc0A/OvX/iFtK7SHrOvlV1wJwHHHZH8l\nXvf4owDs7d8DgFk2ppfj5iEj2SS9cpyA19EeJvm1tmQT5Sxmky1u6lEmnx0OWe+m+BpaWlrStsbG\ncK4QM8aj5SwbPVIK97D8uOMRmQafBH4DuAx4yMy+R1jn+E3AfODj7n73QYz3B8Dbzexu4GlgJ2FN\n5N8iTLD7dNLR3W8xs1XAnwJPm1mymkY3YV3kVwH/CLzjsF6hiIgcdWo2OBaRmc3dh83sEuB9wO8B\n7yTbIe897v71gxzy60AjcB6wirA5yEbgVuCv3X1NxfNfZWa3EwLg1xAm/+0gBMmfAL56iC9NRESO\nYjUbHHusHaYht1RasRWAF7aEjToWtmWbcozE7Z83bA51wnsHsvrgob2DACzpCX997VqQZWZ/9uBD\nAHznS18EYPezT2fPF+cNDe4NmeO+eAzjh+x1IVcD3NgQM8DHLAagvWFO9oJiprmhMWyLTa6ueDSu\najUY73OoP5vDlKxglWxA4rml40aT+UbnvgqRw+Xu6xlnz3N3v7DKuUHC8msfnYTxf0bYOW/C4nbW\n3zlgRxERmTVUcywiIiIiEik4FhERERGJarasIpmPZ7m/wi5aFCbkre4NZRW/fCjbBa9nbg8AP/j3\nOwF49sncylKlUK4w3N8HwOIF2XJtF7/qbAC2bg77Hjx093+mbfMWhp34jovLvS3u7knbkolxTXFn\nPoCuzjDRvqU5TKIbHc42AXMPn2Na20JpSHNHdl2xPvxnHI7Lwu3Zm+2Gu3v3XgA2bdwKQG987QAN\nTVl5iIiIiIgocywiIiIikqrZzHEdcaLbcLZ02TELugFYuuxYAG774u1p21BfyAq/uDHsWlvqy3bA\n7WwNY/X2hezrs5uyrG173EDjmGPD5LkFMVsMUIzLrs3rCZPoTjrphLStrbUt9sk2KSFu9NHXG547\nvxFJ786wT8HceXMBsFJ32lYfJ+kNlULmuKmYLQHXOTfc36I5JwPw/Avb07ZyfdZPRERERJQ5FhER\nERFJ1WzmOKk1TpYyAxiOmdUVp6wEoHvBvLTtl+tCjXFrS8jkDgxnS6x1zQmZ4uaWkKHd07czbauv\nC+MfG+uKW5qzTPDCRSGLPGdOqCUeGszqfYnLr9UVss8npbghiMUl4Hp6sszutq0hm7x9axijsTiY\nva66MMbAYNgArOzZmI2N4d475oRNwk4745S0rWnuMYiIiIhIRpljEREREZFIwbGIiIiISFS7ZRVx\nBbf8DnQj5bDr3fy4A91Fr7skbRvo3QbA9o0bwnU0p20NbXHnupb47RrJShoai2ESXXNjLOMo96dt\nw4Nhkl9ppBj7tGb3x74714WLR5MHAHR1NKZNJ5+0FIDe7WFC3ejebMJgsSGM74NhB76RuKQbQGlv\nKAHZszNMNBy2bOe/Hov33vESRERERESZYxERERGRVM1mjhNm2SYgSV62oSlMrDt+xYlp2/LjlwFQ\nLIXM747tWfa1WB/GsELMtNZnk+662kOGuVAIGdmOttzmHPHcQH/I8pZHs6xyc3PYgMNyk+d8dDRe\nl2SVsyzvaJys1xQ3/BjanU3u698dxq1vKsYxs+tKcSOR0XJ4DTvWP5K7h7b44l+JiIiIiChzLCIi\nIiKSquHMcYj7PR//W6zNLYcMcENDVlfc0hk2y+iYHzbXKNdltcA+HDKzbZ2h/9JFc9O2RfPD5h9z\n2kN9cHNL9i1tjMu6eTlkhAd370rbBneFbHIhW2kOj1nupE56aCjbPnpoMNQ51yc11HVZRnxkMIzf\nWAj30N6SZa9LHscaCa/HR4ez64ayumURERERUeZYRI4yZrbezNZP932IiEhtUnAsIiIiIhLVbFlF\n3LiOhvyEPI+P41Jnu3fuSNtKcRJbx7ye0Lc8ml03EL5N8+Z1ATBnblfaVt+QLBUXnjDZhQ+gnrDD\nXbEYri+RTZQb7A/LrpVHstKJujjRrxx39Rsdze6hYPFzTHw9xaZsmbdyfziX7LDXM6c7besfLCUD\nALBhW7a7X+8zzwJw0kWIyBGwZuMull3z3apt6z/2+im+GxERmQhljkVEREREoprNHI8OhaXO+ndu\nSc/t2Bk2+nhx03MArH8iW9aMmDluaQlLrJU6OtKmcpz81tAYsrW7dmUT63b3hesWsBCAjp4soztc\nChngYszaDg1nk/wGRpLscHYLDR6yzoW4GUhjbsm4QjF8jhkaCc+3ozdbFm7DumcA6GwOWeyRPdn9\n7ekLG5HUNYUNSB56LJuEN9wa2n7zbYjMKBbWYLwK+BPgeGA78C3gujH6NwLvBa6I/UvAQ8DN7v7P\nY4z/LuDtwHEV4z8E4O7LJvM1iYjI0aFmg2MROap9mhC8vgB8ARgBLgPOBhqAdNkVM2sA7gAuAB4D\nPg+0AJcDt5nZy9392orxP08IvDfF8YeBNwBnAfXx+SbEzFaP0bRiomOIiMjMUbPBce+ukCXe1ZfV\nFfduD+f6el8EYLSc/f5r7wzLn5Xi8mmdnZ1p266hkKXdsiVkoa28N20b7g9Z2mSJtHLud2q5HJZ5\na4vZ6DrLMsf1SUHLaG4tt1h/bIR+e4cG0qY9cdj+mH1+YcOGtM2HQ7/utvB8vTFbDLCrN2w33RRX\nd1v79NbsHuZlW2uLzBRmdh4hMH4aOMvdd8Tz1wF3AouAZ3OXXE0IjG8H3uAe/gxkZjcC9wPvN7Pv\nuPu98fyvEwLjJ4Cz3b03nr8W+E9gccX4IiIyi6jmWERmmrfG40eSwBjA3QeB91fp/zbCjNj3JYFx\n7L8V+FD88o9y/f9bbvzeXP/hMcYfl7uvqvaPkMUWEZGjjIJjEZlpzozHH1Vpu5tsJ3jMrB04Adjk\n7tWC0R/G4xm5c8nju6v0vw9yy8qIiMisU7NlFVYXJrM11Tek5zpiqcTwSCiLqB9qStsKI6F0oq4U\nfi+Wi9kScO1toSxiW18oSfBStstcV0eoVxjeExJQG57MJsM1xeuKhVC+0FTMJtglu/QNDmYT65ri\nf426OPlupJDdX333MQB0LpwHwN7du9O2wV3hnnf3h130mpqy19w2/1gAnn8xPM/DT76Qtp3StRCR\nGSipadpS2eDuJTPbVqXvC5V9K8535c6NN/6omW0/iHsVEZEao8yxiMw0ySfMBZUNZlYHzK3Sd6xP\neosq+gEkRfnVxi8CPRO+UxERqTk1mzmuq28G8pt0QFNzyKh2dbUB0FA/P22b2xUzwLvDJhmb1mXJ\nqT17NgHQWAgZ47bu7HfnySedAICVQib3mcd/lbZtfjFM/Ns9FCbd+Wj219qmlnAvxdYsO7zomPB7\nfN6CxQB092S/u70xLC1X9pBxbpqXTRjs3RvutVgO489vn5O2dcx7CQA/WrMmvK7tWab6zMYWRGag\nBwmlFRcA6yrazgfS/6ndfbeZPQ0cZ2YnuvuTFf2TLW4ezJ37BaG04vwq45/DJP5cPPWYTlZrsw8R\nkaOKMsciMtN8KR6vM7N0u0czawJuqtL/FsCAT8TMb9J/LvA/cn0S/yc3fmeufwPw0cO+exEROarV\nbOZYRI5O7n6Pmd0MvBNYY2bfIFvneCf71xd/EviN2P6QmX2PsM7xm4D5wMfd/e7c+D8ysy8Afww8\nYmbfjOP/FqH8YhNQRkREZqWaDY6b20LpxNzurPxgdCSUG5TLoXxhZHgobRuJ6xsP9oUShZaGLKle\nKIUJfBueCpPufDS7jrpQyjDs4VxLT1vatPiU0wCYf+KvhXGSxYYBj2saD+fWMi4TJgFaXdhlr7Uz\nK48oeWyLX89bcFx2f82/BOC5hx8IbYPZZEJ2hnvfuyeuoZxb2ripJbsfkRnm3YR1iK8i7GKX7GB3\nLXEHu4S7D5vZJcD7gN8jBNXJDnnvcfevVxn/TwhLrb0deEfF+M8T1lgWEZFZqGaDYxE5erm7A5+L\n/yotq9J/kFASMaGyCHcvA38T/6XM7ESgDVh7cHcsIiK1omaD46FSyJTuGcp2rBseClnevXvDkmeD\nA4NpW1NjmBjX0BImwS0++RVp257+kN0txdVVd27amLZtWP8cAO2tHsfOlnk74ZiTATjntb8LgDem\n5ZMM7d0T7im/JNtguJ+GpjCZsK0jW33Kisl/qpAVritkme3jVr4agNtH/h6AZx6+J3tdDWFSYHL1\n8uNekrYtO+FkRGYjM1sIbI1BcnKuhbBtNYQssoiIzEI1GxyLiIzjPcBbzOwuQg3zQuBiYAlhG+r/\nO323JiIi06l2g+NSSAgltcQAAzFTPBjP7YrZW4AXt4cNPixujlX0LAM8UAjLqD21KWSQNz6ZLfO2\ndFFcfq0jLM22Y5unbRufC+Nv3xqua2jP7qVgYUOQ5o5sedbOuXF5t2IsDLasdtgK+y4sUs5NF+qa\nF+qcz3nN5QDc9kS2OtXaJx8HYGcsbT7jFeenbUuWn4TILPUfwOnAa4FuQo3yE8BngU/Hsg4REZmF\najc4FhEZg7v/APjBdN+HiIjMPFrnWEREREQkqtnMcSGWHdSR/XW0PlYrtLaEl93QmC27Vi6FRh8N\ns+6crKRhTldYUu2Bn4Xd71av/XE2ZvtyALb0h+dZfd8TaduGUthJb/GZobShpXNT2jYQJ/kV65qz\nsepDqQXxL7oN9dl/noaG0NbSEna1a2zOdrerawiPexYvAWDJSaenbXfcESbnnfKKVwLw0rNfmba9\nZOlyRERERCSjzLGIiIiISFSzmePRuGlG2XOT2pJscDlkZsvD2TJv5ZGQMS4WQp/BUiltq4vLqL38\nFWF5t+9+53tp2+a+PgDOXBU2+th610/Stief3QxA344dAHTOyTbdKDfVx/vM7nm4FCYBjsbn3jMw\nmvWPM/AsnSeUXTgSH3c2hQzyiy/uSNt29e2Nrytcv2dXlr1+fn3y6AJERERERJljEREREZFUzWaO\nLWaAzbL4P1mdaTTZzSO3HFoh7qs8GmuOy7m10kZGQ0b3JUvDBhqnnPaytO2FF8IScCe8dAUA845Z\nkrb1DYS64t39IXvbP5xtOjISl5qry23nnGS262NxdD1Zo8Vl3erqQsbZilnbQMw4NzWEjUya2nLb\nVNeH5eFG4/ejuzvbWGS0lNsGW0RERESUORYRERERSSg4FhERERGJarasohB3lKuryy2HVg4lCeVy\nXK7Ns9KJZEJe2pYrqxgYDOUQTcVw/emnnZa2rXnkNgBKcaxzzz8vbfvlL1YDUN/cCMBQKZsAODIS\nJt2NDmcT/0pxIl5S0pHfpKsQyyoaYulEXVO2BNxgfO6mYiihKBeySYj9seSibzCUeJx48slpmz4a\niYiIiOxL4ZGIzChmtt7M1k/3fYiIyOxUs5njYpywVp/bSMM9Zo5jpjXJEgOMxAxrMoGvkJvwlmSh\nk8l6CxcsSNvqY7+f/vhuAE475aVp20M/vx+AzRueB2DuvJ7sXmLmuGxZhtpHYxY5LjVXyGWAi4Xw\nPKXhkAEejUeA0bpwfzv29ALw2CO/SNvmdIYM88knLQNgeCSbFFgfJ+uJiIiISKDMsYiIiIhIVPOZ\n42TpMwAn2Rp6/8yxlUM9sHs4NzqStSVKsRa4tbMzPdfa3gHAo4+sBeCUk07KLog7fDy0+iEAli5f\nlrvBkBUu5JaaS/LEybJtVm0Dk5j1Lo1k9culwZBxfvLRcA9rHnowbVu4aD4AK14aao3Lo9nrGvLh\n/V6jiEyeNRt3seya7+53fv3HXj8NdyMiIhOhzLGITDkL/szMHjGzQTPbaGafM7POca55i5ndaWa9\n8Zq1ZvYBM2sco/8KM/uSmW0ws2Ez22JmXzOzk6v0/ZKZuZkdZ2bvNLOHzWzAzO6axJctIiJHgZrN\nHIvIjPZp4F3AC8AXgBHgMuBsoAHY588aZnYL8FbgeeCbQC9wDvAh4GIzu8TdS7n+lwL/AtQD/wY8\nBSwBfht4vZld5O4Psr/PAL8OfBf4HrD/n5BERKSm1WxwXG0pNyxMQEt3ysuVGHgp9Bstxcl3WdUC\n9Ul1Q0Mo1egf2pM1xuF/bdUrAGjLlVxYMYz14rYXARjJlUI0N7aEB+VsubaknCJ9DftMCrTYPU4Y\nbGzKxoqT++a2hhKPZUuXpm0Lli0Pr6E97JrnuecoVDyfyFQws/MIgfHTwFnuviOevw64E1gEPJvr\nfyUhMP4WcIW7D+TabgCuB64iBLaY2Rzg60A/8Cp3fzTX/1TgPuCLwJlVbu9M4Ax3f+YgXs/qMZpW\nTHQMERGZOVRWISJT7a3x+JEkMAZw90Hg/VX6vxsoAW/LB8bRh4DtwBW5c38IdAHX5wPj+BxrgL8H\nzjCzl7K/jx9MYCwiIrVnlmWO952Il28bjkulESfImWUZ3YZ43VD/bgA2PfGrtK15NPyuXtLdBsCL\nG9alba0NITN73nmrwtcdudLIQhi/vphNGEwm5xWLyT1kn12K8fVYPHpush4j4S/QrV2tAKw4Pfud\n37NwEQAd7aHNcp+H8uOLTKEkY/ujKm13kytlMLMW4HRgG/Ceyr+uREPAytzX58bj6TGzXCmZNbsS\neLSi7f7xbrwad19V7XzMKFfLTouIyAxWs8GxiMxYSe3RlsoGdy+Z2bbcqTmEhVzmEconJiJZUPy/\nH6BfW5Vzmyf4HCIiUqNqODiOS6UVsuxowSszs7n621jfW4ht5LZu9php7t8TMsdtLVm973nnhKSR\nl/oB2Lol+926YH43AD09cwBobMxtulEM3/qm+mysupi9Tu55n3tPz4U+uVJlLC4/d8xxob64a/78\ntK2hOWSMm1taK18W5fwgIlNnVzwuANblG8ysDphLmHiX7/sLd59oFja55nR3f/gg703/U4iIzHL6\nu7qITLVklYgLqrSdD6QzUd19D/AIcIqZdU9w/Pvi8dcP+Q5FRGTWquHMsYjMUF8C/gi4zsy+nVut\nogm4qUr/TwH/ANxiZle6e2++Ma5OsTy3NNs/AtcB15vZA+5+f0X/AmEVi7sm8TVVdeoxnazWhh8i\nIkeVmg2OkxKG+vqslKG+Ptk1Lxzzc3vqiuGvqXXEpVJH0yVT05KL7oZQAnHuvMVpW9nD8mxbt4Zy\nigWbt6Ztbc2hpHHxsWFptY45PWlbXRyrMVdWUR9380uWbUtKKMK9WvIAqJiQFycMjsZ78biTX3gc\n/jhQGo7nLFtOLr+UnchUcfd7zOxm4J3AGjP7Btk6xzsJax/n+99iZquAPwWeNrM7gOeAbmA58CpC\nQPyO2H+7mV1OWPrtPjP7ASH77MCxhAl7PUATIiIiFWo2OBaRGe3dwBOE9YnfTliO7VvAtcBDlZ3d\n/Sozu50QAL+GsFTbDkKQ/AngqxX9f2BmpwF/DryOUGIxDGwCfkjYSORIW7Z27VpWraq6mIWIiBzA\n2rVrAZZN9fOau+afiIhMNjMbItRP7xfsi8wQyUY1j03rXYiM7XRg1N0bD9hzEilzLCJyZKyBsddB\nFpluye6Oeo/KTDXODqRHlFarEBERERGJFByLiIiIiEQKjkVEREREIgXHIiIiIiKRgmMRERERkUhL\nuYmIiIiIRMoci4iIiIhECo5FRERERCIFxyIiIiIikYJjEREREZFIwbGIiIiISKTgWEREREQkUnAs\nIiIiIhIpOBYRERERiRQci4hMgJktMbNbzGyTmQ2Z2Xoz+7SZzTnIcbrjdevjOJviuEuO1L3L7DAZ\n71Ezu8vMfJx/TUfyNUjtMrPLzexmM/uJmfXF99NXD3GsSfl5PJa6yRhERKSWmdnxwL3AfODbwGPA\nWcC7gUvN7JXuvn0C4/TEcU4CfgjcCqwA3gq83szOdfd1R+ZVSC2brPdozo1jnC8d1o3KbPYB4HRg\nD/A84WffQTsC7/X9KDgWETmwvyX8IH6Xu9+cnDSzTwHvBT4CvGMC43yUEBh/yt2vzo3zLuAz8Xku\nncT7ltljst6jALj7DZN9gzLrvZcQFD8FXADceYjjTOp7vRpz98O5XkSkpsUsxVPAeuB4dy/n2tqB\nFwAD5rv73nHGaQO2AmVgkbvvzrUVgHXA0vgcyh7LhE3WezT2vwu4wN3tiN2wzHpmdiEhOP4nd//9\ng7hu0t7r41HNsYjI+C6Kx+/nfxADxAD3HqAFOOcA45wDNAP35APjOE4ZuKPi+UQmarLeoykze7OZ\nXWNm7zOz3zCzxsm7XZFDNunv9WoUHIuIjO/keHxijPYn4/GkKRpHpNKReG/dCtwE/DXwPeA5M7v8\n0G5PZNJMyc9RBcciIuPrjMddY7Qn57umaByRSpP53vo28FvAEsJfOlYQguQu4DYzU028TKcp+Tmq\nCXkiIiICgLv/TcWpx4FrzWwTcDMhUP73Kb8xkSmkzLGIyPiSTETnGO3J+d4pGkek0lS8t75IWMbt\n5XHik8h0mJKfowqORUTG93g8jlXDdmI8jlUDN9njiFQ64u8tdx8EkomkrYc6jshhmpKfowqORUTG\nl6zF+dq45FoqZtBeCfQD9x1gnPuAAeCVlZm3OO5rK55PZKIm6z06JjM7GZhDCJC3Heo4IofpiL/X\nQcGxiMi43P1p4PvAMuCqiuYbCVm0r+TX1DSzFWa2z+5P7r4H+Ersf0PFOH8Wx79DaxzLwZqs96iZ\nLTez7srxzWwe8I/xy1vdXbvkyRFlZvXxPXp8/vyhvNcP6fm1CYiIyPiqbFe6FjibsObmE8B5+e1K\nzcwBKjdSqLJ99P3ASuAywgYh58Uf/iIHZTLeo2Z2JfB3wN2ETWl2AC8B/guhlvPnwCXurrp4OWhm\n9kbgjfHLhcDrCO+zn8Rz29z9z2PfZcAzwLPuvqxinIN6rx/SvSo4FhE5MDM7FvggYXvnHsJOTN8C\nbnT3nRV9qwbHsa0buJ7wS2IRsB24Hfif7v78kXwNUtsO9z1qZi8DrgZWAYuBDkIZxSPAPwP/292H\nj/wrkVpkZjcQfvaNJQ2ExwuOY/uE3+uHdK8KjkVEREREAtUci4iIiIhECo5FRERERKJZFxyb2Xoz\nczO7cLrvRURERERmllkXHIuIiIiIjEXBsYiIiIhIpOBYRERERCRScCwiIiIiEs3q4NjMus3sU2b2\njJkNmdlGM/t7M1s0zjUXmdm/mNlmMxuOx2+Z2avHucbjv2VmttLMvmxmG8xsxMz+X67ffDP7hJmt\nMbO9ZjYY+91rZh80s6VjjD/PzG4ys1+Z2Z547Roz+0i1rUBFREREpLpZtwmIma0HlgJ/AHw4Pu4H\nikBj7LYeOLPKjkIfBq6LXzqwi7ClZrLD0Mfc/f1VnjP5Jv8hYWvOFsKuQ/XAHe7+xhj4/pSwYxbA\nKNAHdOXG/xN3/7uKsc8nbJ+YBMHDQBloil9vIGz3+fg43xYRERERYXZnjm8GdhL24G4F2oDLgF5g\nGbBPkGtmv0sWGH8OmO/uc4B5cSyAa8zs98d5zr8FHgBe5u4dhCD56th2PSEwfgp4FdDg7t1AM/Ay\nQiC/ueKelgL/RgiM/xdwYuzfGq/5PnAs8C9mVpzIN0VERERkNpvNmeMtwCnuvr2i/Wrgk8Az7n5c\nPGfAE8AJwK3u/pYq434NeAsh63y8u5dzbck3eR1wqrsPVLn+UWAl8LvuftsEX8tXgSsYO2PdQAjG\nTwPe5O7fmMi4IiIiIrPVbM4cf6EyMI6SGuDlZtYaH7+cEBhDyOBWc2M8LgPOGqPP56oFxlFfPI5Z\n75xnZi3AmwglFJ+q1sfdh4EkIL5kIuOKiIiIzGZ1030D0+iBMc5vzD3uAvYCZ8avX3T3R6pd5O6P\nm9lG4JjY/74q3X46zv18Dzgb+CszO5EQ1N43TjC9Cmgg1D7/KiS3q2qOx2PHeW4RERERYXZnjndX\nO+nug7kv6+NxXjxuZHzPV/Sv9OI41/4V8K+EgPdPgR8CfXGlir8ws66K/kmG2YAF4/zriP1aDnDv\nIiIiIrPebA6OD0XTgbuMa3SsBncfcvfLgHOBjxMyz577+gkzOz13SfLfbpe72wT+XXipChlOAAAg\nAElEQVSY9y4iIiJS8xQcT0yS8T1QacKSiv4Hzd3vc/e/dPdzgTmESX7PEbLRX8x13RKPHWbWeajP\nJyIiIiIZBccT82A8tppZ1cl2ZnYSod443/+wuPted78V+ON4alVukuDPgRKhrOLSyXg+ERERkdlO\nwfHE/JKw/jDAtWP0uSEe1wP3H+wTxGXXxpJMyjNCTTLuvhv4Zjz/QTNrH2fsOjNrO9h7EhEREZlt\nFBxPgIfFoD8Qv7zMzG42sx4AM+sxs88Syh8APpBf4/ggrDGzj5rZK5JA2YKzyDYZeaBi175rgB3A\nScC9ZnapmdXnrj3RzN4HPAb82iHck4iIiMisMps3AbnI3e8ao0/yTVnu7utz5/PbR5fJto9OPmQc\naPvofcar6NMbx4IwcW8X0E62YsY24GJ3f7jiulcQ1mZeHE+NENZMbidmmaML3f1H1Z5bRERERAJl\njg+Cu38AuBj4NiFYbQO2E5Zge021wPggXAbcBNwDbIpjDwMPAx8j7Ob3cOVF7v4AsAL4S+BeYA9h\nfeZ+Ql3yZ4ELFBiLiIiIHNisyxyLiIiIiIxFmWMRERERkUjBsYiIiIhIpOBYRERERCRScCwiIiIi\nEik4FhERERGJFByLiIiIiEQKjkVEREREIgXHIiIiIiKRgmMRERERkUjBsYiIiIhIVDfdNyAiUovM\n7BmgA1g/zbciInK0Wgb0ufvyqXzSmg2Ob/jA9Q5QGt6dnnvDb/8OAL96dC0AD/z0rrStq70FgNdd\n8psAnP2qi9O2ltZWANwdgCefWpu23Xf/TwCY0zkXgFNPXZW2rVu3LvR/cg0AZR9K25Ys6gRg/sIl\n6bkTTwzX9nQvOODrGxjYkz0eHAj30NUDwK6+HWlbY0MTAM3NbeMNZwd8QhE5WB3Nzc3dK1eu7J7u\nGxERORqtXbuWgYGBKX/emg2Ou9pDxcjgcEt6btnx4YNH794+AJ78WTltO769CMDLTj0GgIbG7Fsz\nWg79RobCf6Btm9enbds2hMC3b1MIoBfMzX4Pnnj8MgCGh0Mg+8Lm59K2waHhcF3f3vRcS0t7eL7R\nUQAKhf2rXsySODaLZ4eHh+IxjLm7L/tAUOxK+o0bHIscNjNbBjwDfNndr5zWm5kZ1q9cubJ79erV\n030fIiJHpVWrVvHggw+un+rnVc2xiIiIiEhUs5ljEZHptmbjLpZd893pvg0RkUm1/mOvn+5bOKJq\nNjhe+dIVADz8q0fScy9uexGAYmEwnOgfTtsK/SUAhnf9AgA7Zl7aZoWXhO792wHYu2tj2tZWF0sZ\ndoU634d+cmva1nHpWwA499zzAHhu08K0bdvmTQB0d89NzzXU14f7KxYP+PoaG5uzxw2hNGNkOJR9\nFMhel+Uei4iIiMj4VFYhIpPOzJaZ2a1mts3MBs3s52b2m1X6NZrZNWb2KzPrN7M+M/uJmf3XMcZ0\nM/uSmZ1kZreZ2VYzK5vZhbHPcWb2BTN7yswGzGxHHPvvzKynyphvMbM7zaw33udaM/uAmTUekW+M\niIjMeDWbOS6NhEl0u7b3pufmtIUJby/07QKgq5R9Nnjx2X4Avn/LTwF46fn9aduyM8PKFS09YbJe\nd8+itO0ZD9neE1acAkBrWzbxbe7c0K+zfQ4AK086I23rm38sAIOD2aoTVggZ47goBjbOGhJmnj5u\nbAiTDnfu3BIbR9O2lubOsQcROTKWAvcD64CvAN3Am4Fvm9lr3P1OADNrAO4ALgAeAz4PtACXA7eZ\n2cvd/doq4x8P/Ax4AvgnoBnoM7NFwAOE5dO+B3wTaAKWA38AfA7YngxiZrcAbwWej317gXOADwEX\nm9kl7l460Is1s7Fm3K040LUiIjLz1GxwLCLT5kLgBne/MTlhZl8D/h34C+DOePpqQmB8O/CGJBA1\nsxsJwfX7zew77n5vxfjnAzdVBs5m9k5CIP4ed/9MRVsrUM59fSUhMP4WcIW7D+TabgCuB64C9hlH\nRERqX80Gx+XSNgDaPVvWrK831Apv2xDamnJFJfUt4Vux69mQWPr51h+nbc8/thWAJWeEdYjbF2W1\nw6e/4kIAWppDtnbOkpPTtpaOUE9ssXplZChb57h/IGSmG+uyLK+Xw/2VC2GdY/Pcfx4rx7GS7HKW\n0KpvDNnrOuKYre3ZZYW6OHZ47rJnz2fEGuXifn9tFjkczwIfzp9w9zvM7DngrNzptwEOvC+foXX3\nrWb2IeCLwB8BlcHxFuBGxrbfopjuvrfi1LuBEvC2fGAcfQj4M+AKJhAcu/uqaudjRvnMA10vIiIz\nS80GxyIybX7pnvsUltkAnAtgZu3ACcBGd3+sSt8fxuMZVdoecs/tqJP5V+CjwOfN7HWEko17gEc9\n2cEnPHcLcDqwDXiPVa9fGgJWVmsQEZHapuBYRCZb7xjnS2STgJNi+BfG6Juc76rStrnaBe7+rJmd\nBdwAXAr8dmzaYGafdPfPxq/nEHbRmUconxAREUnVbHD87JNhSbbiyNb03M9/chcA6x/eAEBra7Yc\nWltnmNRWJpYfjGaJr75n1gOwbnso0eh6ydK0bfHLTwznFi2O42RbPzcmWzYXQmaqd1e2rfOP/+MH\nAPz6Ocen54ZGw+986zg1XN/+srTNkv9Unhzq07ZyXPmtfzD8Zdrqs0xYuRwm/A3HHfmKxQ1pmxfD\nucbiKxGZYrviceEY7Ysq+uV5lXOhwX0t8GYzqyNkh18DvBP4jJntdfd/yI35C3dX2YOIiOyjZoNj\nEZm53H23mT0NHGdmJ7r7kxVdLorHBw9x/BKwGlhtZvcCP+b/b+/egyS9yvuOf5++Ts99r9qLkFYS\nErIkB6ylgEBiyXHAJhQXlyEkJClwyhUgOFyTKizHjiAFpgzGSsAu7DKyE9sFxHZhqgBFxMYQFkIc\nCyx0RUIwkna197lf+vqe/PGcfk8zzMzO7s7uzvb8PlVbPXrP2+c970xr5vTTz3kOvAb4ZAhh3swe\nAm42s+0hhMm1+joft+wf474+L5YvItJv+nZyPHvKy5oNVtKxoxO+8UZxwTcDyQYHU2PHI8YhLmhf\nqqf1O8WWt3U6HrBqNNPGGiePTgCw7+abALj6x1P1puL+UQCqVf8k+XvfSRuSPPO0f/I8e+3J/Nix\nh+8HoDzm43zOc/PF9VjV+6rU/NqhmaLD9RlfiLdw6vsAFJrpb33D/Ed84oRfb6SSAnGjYzFynoLd\nIhfT3cAHgA+b2c9385TNbCfwqz3nrIuZHQS+F0JYHm3uvsIXe459FPgkcLeZvSmE8EOpIGa2Dbgm\nhHBOk3MREbl89e3kWEQ2vY8ALwdeDdxvZl/E6xy/DtgN/EYI4dBZ9PevgDeb2SHgCWAKr4n8SnyB\n3V3dE0MId8fJ9L8FnjCze4Gn8FJw1wA/CfwB8JbzukMREbnsaHIsIpdECKFpZi8F3g28Ac8NbgP3\n47WKP3WWXX4KqAIvBg7im4McAT4N/GYI4cFl13+bmd2DT4D/Mb74bxKfJH8Y+ONzvDUREbmM9e3k\n+MhpX5x24Opr8mM7Bjw1Yfb4UwAEG8jbOnFXudmsDsDcfPpkdnzUn9du+eK24nxKdyi2Pb3hyfv+\nHwDHH3s4byuUPDVj8KorAZivpwWAlbanajz87SfyY3mqRM3TIsIz9+RtNrwLgCtv8XNmp1I6RivW\nTJ6OHwxPTqY6x52p2XjMz398bjaNoeD3fNs/ex0i5yuEMIFXgVit/fYVjtXx8msf3ID+/y++c966\nhRA+D3z+bJ4jIiL9rXDmU0REREREtoa+jRw/OeOR4xvG9+bHarOHAZgMHvkt1k/lbe3gAanxMX9s\n9yzdabc9mlwq+YK8dr2Vt3WyKgBZuxnPTXsTzJ70CHA582pV1T3b8rZqeBKAY40xkt3xiR6hXjg2\nlbdYzcvIPT3vKwyb7RS9bjd8g6920yPhYSDtnteZ8/MmT/r7oKPz6WpzC34/tyEiIiIioMixiIiI\niEiubyPHtzzvBQDUyimveKblUdQsljerFFNJtutu9QjzTbfuB+DoUyk399AXHvPntT0iWyymDUKy\n2Ge7HaPKzRQ57iZHVuIX04spHH386GkAqtV6fqxW2w5AIfP3LJOtdH5j3s8rL3je8nVX7c/bFp/y\nvtoN30SkNj6atz2z6BuRNGZ8fDsr6Uc+16kiIiIiIokixyIiIiIikSbHIiIiIiJR36ZVjIx4ysCR\nk2nRXaExBEDdPDVh7/U3521Hxny3vMVpfxzYk9IWyjd4GsXSA152rVJJ1aQGBoreZ91TLkLPGILF\n9x4df/6OXWlBXumEp1DMTqZFdydnvfTbrpr/WArV1Ftt3Bf17TpwFQBXx0eAp0552sfxU96X75wb\nxxAXH47UvLzbnp0pJeTBnmuLiIiIiCLHIiIiIiK5vo0cZ3gU9sjkRH6s2hwHYO+1NwEQduzI2x54\n+O8A2H6FR453Hdidt4Vr/bz28bjw7eTpvK027FHkctEj1fOLaZEfJY8qt5team330FDetPvZvjnJ\nsafL+bFKwyPFwzV/3vBYJW8rDPsYfuygLzTctW04b5t8yL8+8aS/12nXU3R4adoj56XxfX4v5e15\n2xt+Lo1HRERERBQ5FhERERHJ9W3kuBHzfDu2lB8rDfmGG3MNL4vWPP503jY04NHXrOMbfJTb03nb\nwJi/hxh4kechT301bS3davv5iwv+2FxKW0TX4vXaM97X4NJcGsuI5wBXxlIecm3JI8fVko/9+ptv\nyttOn/I+OnHMp1MqNQtz3la2WBauUMzb2ouec9ws+oYkS4spl7o6mK4tIiIiIooci4iIiIjkNDkW\nEREREYn6Nq1icckXxtXGUopBIfPUh4XGCT+nkVIuBmNaRcd8Yd2xiVQOrVTy9xCVUW8L+56Vt80+\nfhSA+Sn/VpZqY3lbVvbnFeu+gO/IfX+Zt9WuuD6OM5VrazQ9BaI65AvxfvCdb+Vtzbjz3uSDvgte\nY+pk3laP6Rql+NMsVgbTGDre5+K03/PsdNox8PjsTgCuQ+TyYWYTACGEA5d2JCIi0o8UORYRERER\nifo2ctxdpNYppOhwIXQjqh49rQxV87aMVnz00mqFUooAh7Yfa837Y2cklVibmPbI8WjTF9Ht2bmY\ntw2Ne+k4Ot5XvdXK25qnjgAwN9fIj03Nex/ZuJ9fGE/XKZfiec1ZP2dpNo1vwb8uxecNVFN5uFLc\niGRxzr8PC/Np44+leURERESkR99OjkVELrUHj8xw4L1fOO9+Jj70ig0YjYiIrIfSKkRk0zH3S2b2\nkJnVzeyImX3czMZWOb9qZu81swfMbNHMZs3sa2b2T9fo/x1m9vDy/s1sopvXLCIiW0/fRo6zuPjO\nCpYfa7V9kV6p4OkUBUbztk7b0w5aRU9fqJPSMSo1X9QWLPZZTDvQjez2usZDJz2dIoSUJhE6vtiu\naD6GgUpafJeZj2WxnnbUawZfPFifj0WMB9P4QubPze+mkhbWWTvWbW74+FKlZSgPxHtt+mO7nFIu\nji0UEdmk7gLeDhwFfg9oAa8GXghUgPx/HDOrAPcCtwGPAr8NDAKvBT5jZs8LIdyxrP/fBt4KPBP7\nbwKvAl4AlOP1RERkC+rbybGIXJ7M7MX4xPgJ4AUhhMl4/FeAvwb2Ak/2POU9+MT4HuBVIYR2PP99\nwN8Av2xmnw8hfCMe/4f4xPgx4IUhhOl4/A7gL4F9y/o/03jvW6XpxvX2ISIim0ffTo4H4w50A7UU\nR52b9YjvXMNLn2XtFMmthhgNNo+mLi6khXVZxdu6QdfqQFrIt22HR3AXj3qgqbFUz9tsznfSG9rm\nY6gNpKjtVTcfBGDq/iP5sVMPP+hjLvpiwunm8byttG0vAKMVv051qCdyXPF7nT3l16v0jr3oi/oK\nmd9Xo5QW+c03FDmWTekX4uMHuhNjgBBC3cx+GZ8g9/rXQADe3Z0Yx/NPmNl/Bn4f+EXgG7HpjT39\nT/ec34z9H9rQuxERkctK306OReSydWt8/OoKbYeAPK/JzEaAZwNHQgiPrnD+l+PjT/Qc63690iT4\nm0B7heOrCiEcXOl4jCjfulKbiIhsXn07Oa7EMG9tMEVYSwMxbzeWTKtV0+3Xyl7mrVz0rN6sk6Kq\nRYsR3yxu6lFMfTbj8041PFpbqacSaztjGbVmy/OQG4Opz/1lzyceGl/Ijw1X/W9y/YRvGlIf3Za3\nje3wiG+jEQNdpZRLvRDHNzMfc6PDTLrnasw5HvBrz8+m+m3DjCOyCXUX3R1f3hBCaJvZqRXOPbpK\nX93jvS/2tfrvmNnpsxiriIj0GVWrEJHNpvvu7orlDWZWAnaucO6eVfrau+w8gO472JX6LwI71j1S\nERHpO5oci8hm0903/bYV2v4BkH8EE0KYwxfu7Tez61c4/6eW9Qnw7Z6+lnsRffyJmoiInFnf/hEY\nG/VPThudufzY4IjfbnXIP2EtWUqPKISYOtGJu9T1pC1UKr6grhWLR/WmXNR27QOgfO2zATjx0P1p\nDHVfnGctT5eYXkzf7tOzsVRcIVWMuuHg3wPgW/f4J8H1Zlr4V52LaRtj3kf5qn15WzEuumsc9rVL\nJybzNUyMb9vuY+j4+6ATzZROOXjjtYhsQn+IL6D7FTP7XE+1igHg11c4/27gA8CHzeznQ/DVtWa2\nE/jVnnO6/ju+iK/b/0w8vwJ8cCNv5Jb9Y9ynDTxERC4rfTs5FpHLUwjh62b2MeDfAQ+a2Z+R6hxP\n8aP5xR8BXh7b7zezL+J1jl8H7AZ+I4RwqKf/r5rZ7wH/BnjIzP489v9KPP3iGSC7gLcoIiKbWN9O\njoeHPRJcn+o96tHgknmktVxMZc3oWHyIfxPb6VsTWr6Qr1DxhXXNdirJFmJUecdN/oluCGlTD5v3\n1Mbt2zyFsV5KZeWebngZ1aGeRXrlzNu3Pd8jyO1aimwXqx5prsZI9dBYSpfsxGj31NgQAJOn8upU\n7I5l5xptj1AvLKTNTarlFB0X2WTegdchfhvwZuA08FngDuD+3hNjCbaXAu8G3oBPqtvxvHeGED61\nQv9vxTcMeTPwlmX9H8ZTNUREZAvq28mxiFy+QggB+Hj8t9yBFc6v4ykR60qLCCFkwG/Ff7mYtzwM\nPHJ2IxYRkX7Rt5PjLO7+Wh3uXXPo+baWecQ4y1KUt9X0KGqh6N+SgvXuHuuR2RC3lC4MpLxdyzwy\nW6x4Sbcrbk5rgsJDD3jfbS/XVhhMEedmXAu5lKW+5uoe8a3d4CXchkdSKbfF4CHw2YaPrz6bQuIj\nMfo8vMs3Azn9eIoI1xd9zN2SbsVC2iCkkKUNS0S2EjPbA5yIk+TusUF822rwKLKIiGxBfTs5FhFZ\nwzuBf25mX8FzmPcAPw1ciW9D/aeXbmgiInIpaXIsIlvR/wKeC7wM2I5/rPQY8F+Bu2Jah4iIbEF9\nOzleiikDiz3pEeWO326p5W0FSwvyWnHRXTHzT1kLhXyHWhqxr6zpaRGlTvq7WTZfpNdZ8jSJMDCY\nt3XGvGRcNn0SgErPWOpxXdxkPfVVjKkPWRxDfSntnpdZdzGhn9MppMX0p+PufLZr2PsZGcnbZmNZ\nt6Gaj726Yzhvm5tPu/mJbCUhhL8C/upSj0NERDYfbQIiIiIiIhL1beSYzBepFUnR4Sz44rdONzqc\nqqhRjad1S7EVS+l5wfzb1An+hIWlVA5tsOLnlYoeAa6kam3MjngptqVjfr1aSNHoZndzkkr6ERTL\n/l6lELwTs7RgLsviYkA8AlyrDOVtS/G+GPeIcaeWFv7NTHnkuFTwiHapnJ4XSKXiRERERESRYxER\nERGRnCbHIiIiIiJR36ZVZN2axJ20cK0YYv3fji+Ua7ZSneNgvvgti+vjiksp/aBU8fcQlcGYTlFO\ntYmzjj+v1fRvpRXSArsQsxaOzPpCvNJQqj9cutqPFXoyGwotX2zXTdVYzObztsW6X7NW6qZepB+d\nxfuyzpgPbyAtupvqHANgYMm/D4PV9LzR6j5EREREJFHkWEREREQk6tvIcT0ueGuFFOUNTY+eVuPC\nuko1RXJbHT+WZR7RzZqpjFqr7gvchkseTd5e25+3zc97mTba3tapp+sVK6MAzMaFbycnUym3kbZH\nicuFtIKvVffxzIRTPt5iWjEYZjwq3K74oj4bSjvdtduN+HxfwFccSO95amM7/ZwFL9vWIT1vLptA\nRERERBJFjkVEREREor6NHLdiADe00i12zKOmIb4lyFqp5Fm1NBwfPTLb7qQob6XmZdA6mUejZyan\n8jYLsbRa3PtjMUuR41LZ24a2xcallENcq/j15k6n84n50UsNjyCXSbnDQzFqXawsxXtJZd46FR9X\noz0DQLOd+txz5TY/ZzLe+7ZUom6hkKLjIiIiIqLIsYiIiIhITpNjEdk0zOyAmQUz+8N1nv+meP6b\nNnAMt8c+79yoPkVE5PLRt2kVpXK8tU41P2bmC+OK8S1BMUu3n3UKP9QGqcxbOe5il5d+s55UiJj6\nMLPoKQ2jQzvzlk7J0yjGtvvzx+Z6xtL21ImsmVI7rOCpHJWY4lEglaEbGvbzCmVPizg+1bPbXqwZ\nNzzofRayRt5W2uZttTFfRLg43rNjYOpeREREROjjybGIbAmfBb4JHL3UA1nJg0dmOPDeLwAw8aFX\nXOLRiIjIevTt5HggRnsXsqX8WDGWT6vGTT0WG6ktNHzzjvGKn1MZTLtzNOsepW0u+bFCTzbK0OCI\nX68cS7KFtJBvYdL7X5jxSPMVIyN5W63kUd7WSPoRlAb9vEbckKRUThHgduG0H4vl58aqqcxbq+7R\n4PaSL8yrVtN93XDrP/K2GAl/Yu7beVunnaLjIpejEMIMMHOpxyEiIv1DOccisimZ2Y1m9hdmNmlm\nC2Z2yMxetuycFXOOzWwi/hs1s4/Gr1u9ecRmdoWZfdLMjpvZkpn9nZm98eLcnYiIbFZ9GzkuBo8E\n20LKsW0X41bPcRvoIin6SuZfW8cjwIGUCxxi7m+56Mes5z1FqeCR3MaiX+/Y9OG8bWHGI7+DcYOR\nhYUU0S3M+qYcNtYTaW565LjejLnHPZuUELeIHhz2H1kIaTOP4pJvEDJ72Mc+uH1v3ja2w79++ujj\nPt64xTTAAD17V4tsLtcA/wd4APhdYC/weuAeM3tDCOEz6+ijAnwZ2A58CZgFfgBgZjuBbwDXAofi\nv73AJ+K5IiKyRfXt5FhELms/CXwkhPAfugfM7OP4hPkTZnZPCGH2DH3sBR4GbgshLC/q/UF8YnxX\nCOFdK1xj3czsvlWabjybfkREZHNQWoWIbEYzwPt7D4QQ/hb4E2Ac+Ll19vOe5RNjMysD/wKYA+5c\n5RoiIrJF9W3kuNPwVIbQW3Wt5WkE9YYvRKsOpMas4CkQSw1/fNa+a/O2RnsSgKcPPwVAyVJqQrM1\nDcDMjKdCNOqpPlopVm4bG9wOwPhQStWYw88vFkN+rFr0MRdLMcWjlFInCgU/LzR9pzxrp5SI+rSf\nf3rS++yMj+ZtE4eP+H1lfg8DhZ4FgI2etBKRzeVbIYS5FY5/BXgj8BPAfztDH3XgOyscvxEYBL4W\nF/Stdo11CSEcXOl4jCjfut5+RERkc1DkWEQ2o+OrHD8WH8dWae91IoQQVjjefe6ZriEiIltQ30aO\nC+0dAJRKk/mxYB4VrtXiZiA95w+O+8Yb8zP+t7TVSn9T6y2PyGaZv5dotNPCumbTy7xZwR/37Ezl\n2pod3wSkOeMR43plR942OeWl2YayFE0e3umhZlv0kTWKqdRaNuzple04rqFC6qsSy8Fdc4Mvvit0\n0p19/6lH/dhw3Z8X0kYkVVPkWDatK1Y5vic+rqd820oT497nnukaIiKyBfXt5FhELmu3mtnICqkV\nt8fHb3PuHgUWgeeZ2dgKqRW3/+hTzs0t+8e4T5t/iIhcVpRWISKb0Rjwa70HzOz5+EK6GXxnvHMS\nQmjhi+5GWLYgr+caIiKyRfVt5Hip7gGncrWTHysN+HuBQqzv215MdYQX2546UcZrBU+dTOmInaK3\nNRuemtBspsXvo6PbABga8oVyk8dP5G3NlqdfWCxlfGpmKg2wm9FQSu9PsiU/mAVf1DdQHk9jiHWO\n52b92vNLaXzFgo953+5n+311pvO2UF/4oftqzqc0jsVWHZFN6n8Dv2hmLwS+TqpzXADevI4ybmdy\nB/DTwDvjhLhb5/j1wBeBV51n/yIicpnq28mxiFzWfgC8BfhQfKwC3wLeH0K493w7DyGcMrOX4PWO\nXwk8H/gu8FZggo2ZHB945JFHOHhwxWIWIiJyBo888gjAgYt9XVt5MbeIiJwPM2vgnxHdf6nHIrKK\n7kY1j17SUYis7rlAJ4SeagIXgSLHIiIXxoOweh1kkUutu7ujXqOyWa2xA+kFpQV5IiIiIiKRJsci\nIiIiIpEmxyIiIiIikSbHIiIiIiKRJsciIiIiIpFKuYmIiIiIRIoci4iIiIhEmhyLiIiIiESaHIuI\niIiIRJoci4iIiIhEmhyLiIiIiESaHIuIiIiIRJoci4iIiIhEmhyLiKyDmV1pZneb2TNm1jCzCTO7\ny8y2nWU/2+PzJmI/z8R+r7xQY5etYSNeo2b2FTMLa/wbuJD3IP3LzF5rZh8zs6+Z2Wx8Pf3xOfa1\nIb+PV1PaiE5ERPqZmV0HfAPYDXwOeBR4AfAO4GfN7CUhhNPr6GdH7OcG4MvAp4EbgV8AXmFmfz+E\n8P0LcxfSzzbqNdrjfascb5/XQGUr+4/Ac4F54DD+u++sXYDX+o/Q5FhE5Mx+B/9F/PYQwse6B83s\no8C7gA8Ab1lHPx/EJ8YfDSG8p6eftwP/JV7nZzdw3LJ1bNRrFIAQwp0bPUDZ8t6FT4q/B9wG/PU5\n9rOhr/WVaPtoEZE1xCjF94AJ4LoQQtbTNgIcBQzYHUJYWKOfYeAEkAF7QwhzPW0F4PvA1fEaih7L\num3UazSe/xXgthCCXbABy5ZnZrfjk+M/CSH8y7N43oa91teinGMRkbX9VHz8Uo+jS8AAAAMfSURB\nVO8vYoA4wf06MAi86Az9vAioAV/vnRjHfjLg3mXXE1mvjXqN5szs9Wb2XjN7t5m93MyqGzdckXO2\n4a/1lWhyLCKytufEx8dWaX88Pt5wkfoRWe5CvLY+Dfw68JvAF4GnzOy15zY8kQ1zUX6PanIsIrK2\nsfg4s0p79/j4RepHZLmNfG19DnglcCX+SceN+CR5HPiMmSknXi6li/J7VAvyREREBIAQwm8tO/Rd\n4A4zewb4GD5R/p8XfWAiF5EixyIia+tGIsZWae8en75I/YgsdzFeW7+Pl3F7Xlz4JHIpXJTfo5oc\ni4is7bvxcbUctuvj42o5cBvdj8hyF/y1FUKoA92FpEPn2o/Ieboov0c1ORYRWVu3FufLYsm1XIyg\nvQRYBL55hn6+CSwBL1keeYv9vmzZ9UTWa6Neo6sys+cA2/AJ8qlz7UfkPF3w1zpociwisqYQwhPA\nl4ADwNuWNb8Pj6L9UW9NTTO70cx+aPenEMI88Efx/DuX9fNLsf97VeNYztZGvUbN7Boz2768fzPb\nBfxB/M9PhxC0S55cUGZWjq/R63qPn8tr/Zyur01ARETWtsJ2pY8AL8Rrbj4GvLh3u1IzCwDLN1JY\nYfvovwF+DHg1vkHIi+Mvf5GzshGvUTN7E/AJ4BC+Kc0kcBXwT/Bczr8FXhpCUF68nDUzew3wmvif\ne4CfwV9nX4vHToUQ/n089wDwA+DJEMKBZf2c1Wv9nMaqybGIyJmZ2bOA9+PbO+/Ad2L6LPC+EMLU\nsnNXnBzHtu3Af8L/SOwFTgP3AL8WQjh8Ie9B+tv5vkbN7MeB9wAHgX3AKJ5G8RDwP4DfDSE0L/yd\nSD8yszvx332rySfCa02OY/u6X+vnNFZNjkVEREREnHKORUREREQiTY5FRERERCJNjkVEREREIk2O\nRUREREQiTY5FRERERCJNjkVEREREIk2ORUREREQiTY5FRERERCJNjkVEREREIk2ORUREREQiTY5F\nRERERCJNjkVEREREIk2ORUREREQiTY5FRERERCJNjkVEREREIk2ORUREREQiTY5FRERERKL/D0GD\nUdqYkrqbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf388923c8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
